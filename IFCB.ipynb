{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IFCB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1ExTR163G6Wr9Jq8eqjQz7sAfdJJSVlqs",
      "authorship_tag": "ABX9TyN/Lt6tCO+EPA/DY73ibtjP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pglez82/IFCB_semisupervised/blob/master/IFCB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RStEOrZz2isi",
        "colab_type": "text"
      },
      "source": [
        "# Showing system info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mdydmkeg2pHa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "cb7c0610-8a3a-4b74-bf8a-b359651cd15a"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun 11 16:14:01 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 13.7 gigabytes of available RAM\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aj37uariCNh",
        "colab_type": "text"
      },
      "source": [
        "# Download SimCLR code\n",
        "In this step we download the SimCLR code for **PyTorch** and install its dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU38uvtmh_HO",
        "colab_type": "code",
        "outputId": "1920696c-3282-407a-a72b-049dec5258b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        }
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir(\"SimCLR\"):\n",
        "  !git clone https://github.com/spijkervet/SimCLR.git\n",
        " \n",
        "%cd SimCLR\n",
        "!sh setup.sh || python3 -m pip install -r requirements.txt || exit 1\n",
        "!pip install  pyyaml --upgrade"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/SimCLR\n",
            "setup.sh: 2: setup.sh: conda: not found\n",
            "setup.sh: 2: setup.sh: conda: not found\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.5.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.6.0+cu101)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: sacred in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.8.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (5.3.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->-r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->-r requirements.txt (line 2)) (7.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 3)) (1.6.0.post3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 3)) (47.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 3)) (1.29.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 3)) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 3)) (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 3)) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 3)) (0.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 3)) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: py-cpuinfo>=4.0 in /usr/local/lib/python3.6/dist-packages (from sacred->-r requirements.txt (line 4)) (6.0.0)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.6/dist-packages (from sacred->-r requirements.txt (line 4)) (3.1.3)\n",
            "Requirement already satisfied: jsonpickle<2.0,>=1.2 in /usr/local/lib/python3.6/dist-packages (from sacred->-r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.6/dist-packages (from sacred->-r requirements.txt (line 4)) (0.4.3)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from sacred->-r requirements.txt (line 4)) (1.12.1)\n",
            "Requirement already satisfied: munch<3.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from sacred->-r requirements.txt (line 4)) (2.5.0)\n",
            "Requirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.6/dist-packages (from sacred->-r requirements.txt (line 4)) (20.4)\n",
            "Requirement already satisfied: docopt<1.0,>=0.3 in /usr/local/lib/python3.6/dist-packages (from sacred->-r requirements.txt (line 4)) (0.6.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 3)) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 3)) (2.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython->sacred->-r requirements.txt (line 4)) (4.0.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=18.0->sacred->-r requirements.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->-r requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython->sacred->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already up-to-date: pyyaml in /usr/local/lib/python3.6/dist-packages (5.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6QhQvaJh75Z",
        "colab_type": "text"
      },
      "source": [
        "# Use of Google TPU [Optional]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXwwF4exiD96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_tpu = False\n",
        "\n",
        "if use_tpu:\n",
        "  assert os.environ['COLAB_TPU_ADDR'] #make sure we are in a TPU enviroment\n",
        "  VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "  !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "  !python pytorch-xla-env-setup.py --version $VERSION\n",
        "\n",
        "  # imports the torch_xla package for TPU support\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "  dev = xm.xla_device()\n",
        "  print(dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofyw_2IYzlMu",
        "colab_type": "text"
      },
      "source": [
        "# Download the images\n",
        "In this section, we **donwload** the data and **uncompress** it. The code has checks in order to ensure that already downloaded data is not redownloaded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA0THo_kbFMw",
        "colab_type": "code",
        "outputId": "8b7a4b1c-c526-441e-f1b5-2b7fe32813c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "if not os.path.isfile(\"IFCB_data.tar\") and not os.path.isdir(\"data\"):\n",
        "  print(\"Data do not exist in local. Downloading...\")\n",
        "  !wget -O IFCB_data.tar https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/Ec2z0uC4lghEg-9MjzoJ9QkBK5n74QjS-LszB9dlNrPfaw?download=1\n",
        "else:\n",
        "  print(\"Data already exists. Skipping download.\")\n",
        "\n",
        "if not os.path.isdir(\"data\"):\n",
        "  print(\"Extracting the tar file...\")\n",
        "  !tar -xf \"IFCB_data.tar\"\n",
        "  print(\"Done. Removing the tar file.\")\n",
        "  !rm -f IFCB_data.tar #Remove the original file to save space"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data already exists. Skipping download.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUG0AmQ6z8us",
        "colab_type": "text"
      },
      "source": [
        "# Download CSV with information about the images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoVqmVot04VX",
        "colab_type": "code",
        "outputId": "7b14fe69-492c-4c16-dc87-cca2e1df375f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "if not os.path.isfile('IFCB.csv.zip'):\n",
        "  print(\"CSV data do not exist. Downloading...\")\n",
        "  !wget -O IFCB.csv.zip \"https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/EfsVLhFsYJpPjO0KZlpWUq0BU6LaqJ989Re4XzatS9aG4Q?download=1\"\n",
        "\n",
        "data = pd.read_csv('IFCB.csv.zip',compression='infer', header=0,sep=',',quotechar='\"')\n",
        "print(data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                        Sample  roi_number  ...       AutoClass FunctionalGroup\n",
            "0        IFCB1_2006_158_000036           1  ...             mix      Flagellate\n",
            "1        IFCB1_2006_158_000036           2  ...     ciliate_mix         Ciliate\n",
            "2        IFCB1_2006_158_000036           3  ...             mix      Flagellate\n",
            "3        IFCB1_2006_158_000036           4  ...             mix      Flagellate\n",
            "4        IFCB1_2006_158_000036           5  ...             mix      Flagellate\n",
            "...                        ...         ...  ...             ...             ...\n",
            "3457814  IFCB5_2014_353_205141        6850  ...  Leptocylindrus          Diatom\n",
            "3457815  IFCB5_2014_353_205141        6852  ...             mix      Flagellate\n",
            "3457816  IFCB5_2014_353_205141        6855  ...             mix      Flagellate\n",
            "3457817  IFCB5_2014_353_205141        6856  ...             mix      Flagellate\n",
            "3457818  IFCB5_2014_353_205141        6857  ...             mix      Flagellate\n",
            "\n",
            "[3457819 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB1gMsg5EIZV",
        "colab_type": "text"
      },
      "source": [
        "# Create training set\n",
        "\n",
        "Here we make a reestructuration of the images depending on which class we consider"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX4-tijiEVcO",
        "colab_type": "code",
        "outputId": "af02982c-ba96-4e17-f0bd-4e8bbdb17e21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import progressbar\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "classcolumn = \"AutoClass\" #Autoclass means 51 classes\n",
        "yearstraining = ['2006'] #Years to consider as training\n",
        "trainingfolder = \"training\"\n",
        "\n",
        "classes = pd.unique(data[classcolumn])\n",
        "print(\"Considering %i classes\" % len(classes))\n",
        "\n",
        "print(\"Creating training set...\")\n",
        "\n",
        "if not os.path.isdir(trainingfolder):\n",
        "  print(\"Create folder structure...\")\n",
        "  os.mkdir(trainingfolder)\n",
        "  for folder in classes:\n",
        "    os.mkdir(os.path.join(trainingfolder,folder))\n",
        "  print(\"Done.\\nMoving images to the respective folders...\")\n",
        "\n",
        "  #Compute data paths\n",
        "  data['year'] = data['Sample'].str[6:10].astype(str)\n",
        "  data['path']=\"data\"+'/'+data['year']+'/'+data['OriginalClass'].astype(str)+'/'+data['Sample'].astype(str)+'_'+data['roi_number'].apply(lambda x: str(x).zfill(5))+'.png'\n",
        "  #Move images to the training directory following the structure\n",
        "  data[data['year'].isin(yearstraining)].progress_apply(lambda row: os.rename(row['path'],os.path.join(trainingfolder,row[classcolumn],os.path.basename(row['path']))),axis=1)\n",
        "  print(\"Done\")\n",
        "else:\n",
        "  print(\"Data structure already created.\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Considering 51 classes\n",
            "Creating training set...\n",
            "Data structure already created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPW_XQETJi6H",
        "colab_type": "text"
      },
      "source": [
        "# Loading the training dataset\n",
        "\n",
        "Use pytorch to load the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvbF4INZJpGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from modules.transformations import TransformsSimCLR\n",
        "\n",
        "#This transform makes the magic and returns two augmented images from an original image\n",
        "train_dataset = torchvision.datasets.ImageFolder(root=trainingfolder, transform=TransformsSimCLR(size=64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd2u3DeVjL1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def train(args, train_loader, model, criterion, optimizer, writer):\n",
        "  loss_epoch = 0\n",
        "  start_time = time.time()\n",
        "  for step, ((x_i, x_j), _) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    x_i = x_i.to(args.device)\n",
        "    x_j = x_j.to(args.device)\n",
        "\n",
        "    # positive pair, with encoding\n",
        "    h_i, z_i = model(x_i)\n",
        "    h_j, z_j = model(x_j)\n",
        "\n",
        "    loss = criterion(z_i, z_j)\n",
        "\n",
        "    #if apex and args.fp16:\n",
        "    #    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "    #        scaled_loss.backward()\n",
        "    #else:\n",
        "    loss.backward()\n",
        "\n",
        "    if use_tpu:\n",
        "      xm.optimizer_step(optimizer)\n",
        "      xm.mark_step()\n",
        "    else:\n",
        "      optimizer.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "      spent = time.time()-start_time\n",
        "      print(f\"Step [{step}/{len(train_loader)}]\\t Loss: {loss.item()} \\t Time: {spent} secs [{(args.batch_size*50)/spent} ej/sec]]\")\n",
        "      start_time = time.time()\n",
        "\n",
        "    writer.add_scalar(\"Loss/train_epoch\", loss.item(), args.global_step)\n",
        "    loss_epoch += loss.item()\n",
        "    args.global_step += 1\n",
        "\n",
        "  return loss_epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh9aOkEfWUEq",
        "colab_type": "text"
      },
      "source": [
        "# Lets configure SimCLR\n",
        "Number of epocs, optimizer, resnet version to use ...\n",
        "Things that we have to configure:\n",
        "\n",
        "\n",
        "*   cuda:0 -> Change to cuda:1 to use second gpu\n",
        "*   args.batch_size -> higher value its slower but better\n",
        "*   args.resnet -> resnet18 | resnet50\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKfV68rAjVeE",
        "colab_type": "code",
        "outputId": "c31e3498-1d7e-4e90-d38a-c575b267cdf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from utils.yaml_config_hook import yaml_config_hook\n",
        "import argparse\n",
        "\n",
        "config = yaml_config_hook(\"./config/config.yaml\")\n",
        "args = argparse.Namespace(**config)\n",
        "\n",
        "#Here we need to select which graphics card we want to use in case of having more than one\n",
        "if use_tpu:\n",
        "  args.device = dev\n",
        "else:\n",
        "  args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using %s\" % args.device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwtMapDQjyfY",
        "colab_type": "code",
        "outputId": "23b05c2e-faa5-4bfa-8987-86bddb7c065d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "from pprint import pprint\n",
        "args.batch_size = 64 #@param\n",
        "args.resnet = \"resnet18\" #@param ['resnet18','resnet50']\n",
        "args.epoch_num =  0#@param #Means that we want to start training in this epoch. We should have a file checkpoint_{}.tar in the args.model_path dir\n",
        "#We want to save the checkpoints to google drive\n",
        "args.out_dir = \"../drive/My Drive/Colab Notebooks/IFCBv2\" #change to local directory\n",
        "args.model_path = args.out_dir #This is the directory from where we want to restore checkpoints\n",
        "if not os.path.isdir(args.out_dir):\n",
        "  raise SystemExit(\"The output folder does not exist!\")\n",
        "pprint(vars(args))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 64,\n",
            " 'dataset': 'CIFAR10',\n",
            " 'device': device(type='cuda', index=0),\n",
            " 'epoch_num': 0,\n",
            " 'epochs': 100,\n",
            " 'fp16': False,\n",
            " 'fp16_opt_level': 'O2',\n",
            " 'logistic_batch_size': 256,\n",
            " 'logistic_epochs': 500,\n",
            " 'model_path': '../drive/My Drive/Colab Notebooks/IFCBv2',\n",
            " 'normalize': True,\n",
            " 'optimizer': 'Adam',\n",
            " 'out_dir': '../drive/My Drive/Colab Notebooks/IFCBv2',\n",
            " 'pretrain': True,\n",
            " 'projection_dim': 64,\n",
            " 'resnet': 'resnet18',\n",
            " 'seed': 42,\n",
            " 'start_epoch': 0,\n",
            " 'temperature': 0.5,\n",
            " 'weight_decay': 1e-06,\n",
            " 'workers': 16}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnEU4CkTWNPi",
        "colab_type": "text"
      },
      "source": [
        "# Prepare the model for the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3n6_SXBTkvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sampler = None\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    shuffle=(train_sampler is None),\n",
        "    drop_last=True,\n",
        "    num_workers=args.workers,\n",
        "    sampler=train_sampler,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPt7HTMDWHGj",
        "colab_type": "text"
      },
      "source": [
        "# Load the model\n",
        "We only reload the model if **args.epoch_num** is different from zero. This case means that we want to continue training from a checkpoint (we should have the model in the **args.model_path** dir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asEk24AyUUyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from model import load_model\n",
        "model, optimizer, scheduler = load_model(args, train_loader,reload_model=(args.epoch_num!=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgnQQ_UTWuNn",
        "colab_type": "text"
      },
      "source": [
        "# Configure TensorBoard\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2sVEpLMWzCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "tb_dir = os.path.join(args.out_dir, \"colab\")\n",
        "if not os.path.exists(tb_dir):\n",
        "  os.makedirs(tb_dir)\n",
        "writer = SummaryWriter(log_dir=tb_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esq_Jrkh_zjB",
        "colab_type": "text"
      },
      "source": [
        "# Load the loss function\n",
        "This function tries to minimize the difference between the two augmented variations of the image and maximize the difference between these and the rest of the batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XqPNffhXVCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from modules import NT_Xent\n",
        "\n",
        "criterion = NT_Xent(args.batch_size, args.temperature, args.device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2RE3UTiAD49",
        "colab_type": "text"
      },
      "source": [
        "# Training the CNN\n",
        "We make a checkpoint each 5 epochs just in case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ibd6L_Gbwnq",
        "colab_type": "code",
        "outputId": "20517c5d-d3be-4734-c6ad-3f3af92dbd63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from model import save_model\n",
        "\n",
        "args.global_step = 0\n",
        "if args.epoch_num!=0: #If we have loaded a model trained til an epoch, lets start training in the next\n",
        "  args.start_epoch=args.epoch_num+1\n",
        "args.current_epoch = args.start_epoch #Variable for controlling in which epoch we are\n",
        "\n",
        "for epoch in range(args.start_epoch, args.epochs):\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "    loss_epoch = train(args, train_loader, model, criterion, optimizer, writer)\n",
        "\n",
        "    if scheduler:\n",
        "        scheduler.step()\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        save_model(args, model, optimizer)\n",
        "\n",
        "    writer.add_scalar(\"Loss/train\", loss_epoch / len(train_loader), epoch)\n",
        "    writer.add_scalar(\"Misc/learning_rate\", lr, epoch)\n",
        "    print(\n",
        "        f\"Epoch [{epoch}/{args.epochs}]\\t Loss: {loss_epoch / len(train_loader)}\\t lr: {round(lr, 5)}\"\n",
        "    )\n",
        "    args.current_epoch += 1\n",
        "\n",
        "## end training\n",
        "save_model(args, model, optimizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step [0/2046]\t Loss: 4.853462219238281 \t Time: 5.148331642150879 secs [621.5605797032723 ej/sec]]\n",
            "Step [50/2046]\t Loss: 4.8086700439453125 \t Time: 9.259265422821045 secs [345.5997699464422 ej/sec]]\n",
            "Step [100/2046]\t Loss: 4.5811238288879395 \t Time: 9.154468536376953 secs [349.5560651373933 ej/sec]]\n",
            "Step [150/2046]\t Loss: 4.57763671875 \t Time: 9.30465316772461 secs [343.91394738924356 ej/sec]]\n",
            "Step [200/2046]\t Loss: 4.797720432281494 \t Time: 9.267889976501465 secs [345.27816019757796 ej/sec]]\n",
            "Step [250/2046]\t Loss: 4.810873031616211 \t Time: 9.477900743484497 secs [337.6275070405029 ej/sec]]\n",
            "Step [300/2046]\t Loss: 4.839878559112549 \t Time: 9.210098505020142 secs [347.4447095496078 ej/sec]]\n",
            "Step [350/2046]\t Loss: 4.455430507659912 \t Time: 9.244152069091797 secs [346.1647943567839 ej/sec]]\n",
            "Step [400/2046]\t Loss: 4.581033229827881 \t Time: 9.174200057983398 secs [348.8042532073798 ej/sec]]\n",
            "Step [450/2046]\t Loss: 4.482931137084961 \t Time: 9.113032102584839 secs [351.14547649759135 ej/sec]]\n",
            "Step [500/2046]\t Loss: 4.48732328414917 \t Time: 9.215355634689331 secs [347.24650104161486 ej/sec]]\n",
            "Step [550/2046]\t Loss: 4.554518222808838 \t Time: 9.240790128707886 secs [346.2907343884724 ej/sec]]\n",
            "Step [600/2046]\t Loss: 4.536438941955566 \t Time: 9.239695310592651 secs [346.33176662561897 ej/sec]]\n",
            "Step [650/2046]\t Loss: 4.38309383392334 \t Time: 9.17167615890503 secs [348.9002385777689 ej/sec]]\n",
            "Step [700/2046]\t Loss: 4.452552795410156 \t Time: 9.187675714492798 secs [348.2926584960182 ej/sec]]\n",
            "Step [750/2046]\t Loss: 4.432003974914551 \t Time: 9.225523948669434 secs [346.86376815069946 ej/sec]]\n",
            "Step [800/2046]\t Loss: 4.298683166503906 \t Time: 9.143265008926392 secs [349.9843870735347 ej/sec]]\n",
            "Step [850/2046]\t Loss: 4.284852504730225 \t Time: 9.175954818725586 secs [348.7375497392037 ej/sec]]\n",
            "Step [900/2046]\t Loss: 4.595937728881836 \t Time: 9.109153270721436 secs [351.2950001934223 ej/sec]]\n",
            "Step [950/2046]\t Loss: 4.311247825622559 \t Time: 9.18604588508606 secs [348.3544541395485 ej/sec]]\n",
            "Step [1000/2046]\t Loss: 4.452326774597168 \t Time: 9.12710690498352 secs [350.6039792579572 ej/sec]]\n",
            "Step [1050/2046]\t Loss: 4.312798500061035 \t Time: 9.197422742843628 secs [347.9235530942481 ej/sec]]\n",
            "Step [1100/2046]\t Loss: 4.372749328613281 \t Time: 9.257562398910522 secs [345.6633465820973 ej/sec]]\n",
            "Step [1150/2046]\t Loss: 4.08242654800415 \t Time: 9.20261025428772 secs [347.72742858571485 ej/sec]]\n",
            "Step [1200/2046]\t Loss: 4.249709129333496 \t Time: 9.10590934753418 secs [351.4201468375631 ej/sec]]\n",
            "Step [1250/2046]\t Loss: 4.267796993255615 \t Time: 9.153853416442871 secs [349.57955457882997 ej/sec]]\n",
            "Step [1300/2046]\t Loss: 4.363705635070801 \t Time: 9.259778499603271 secs [345.58062054476807 ej/sec]]\n",
            "Step [1350/2046]\t Loss: 4.353950500488281 \t Time: 9.14652681350708 secs [349.85957678213094 ej/sec]]\n",
            "Step [1400/2046]\t Loss: 4.4315972328186035 \t Time: 9.054157257080078 secs [353.4288072473776 ej/sec]]\n",
            "Step [1450/2046]\t Loss: 4.295778274536133 \t Time: 9.151634454727173 secs [349.6643157930195 ej/sec]]\n",
            "Step [1500/2046]\t Loss: 4.501316547393799 \t Time: 9.138889789581299 secs [350.1519411743129 ej/sec]]\n",
            "Step [1550/2046]\t Loss: 4.339417457580566 \t Time: 9.120208024978638 secs [350.8691897416995 ej/sec]]\n",
            "Step [1600/2046]\t Loss: 4.370504856109619 \t Time: 9.122594833374023 secs [350.7773893775428 ej/sec]]\n",
            "Step [1650/2046]\t Loss: 4.410672187805176 \t Time: 9.134304761886597 secs [350.3277023722901 ej/sec]]\n",
            "Step [1700/2046]\t Loss: 4.232817649841309 \t Time: 9.15542721748352 secs [349.5194624986117 ej/sec]]\n",
            "Step [1750/2046]\t Loss: 4.3067498207092285 \t Time: 9.10782504081726 secs [351.3462309233005 ej/sec]]\n",
            "Step [1800/2046]\t Loss: 4.259001731872559 \t Time: 9.102152347564697 secs [351.5651988462012 ej/sec]]\n",
            "Step [1850/2046]\t Loss: 4.39562463760376 \t Time: 9.138345718383789 secs [350.1727882282345 ej/sec]]\n",
            "Step [1900/2046]\t Loss: 4.2866716384887695 \t Time: 9.076771259307861 secs [352.5482694871846 ej/sec]]\n",
            "Step [1950/2046]\t Loss: 4.139901161193848 \t Time: 9.119882583618164 secs [350.88171044527337 ej/sec]]\n",
            "Step [2000/2046]\t Loss: 4.178431034088135 \t Time: 9.204678773880005 secs [347.64928560903155 ej/sec]]\n",
            "Epoch [0/100]\t Loss: 4.417419419843314\t lr: 0.0003\n",
            "Step [0/2046]\t Loss: 4.479201316833496 \t Time: 3.433384656906128 secs [932.0249024714772 ej/sec]]\n",
            "Step [50/2046]\t Loss: 4.392343521118164 \t Time: 10.448129653930664 secs [306.2749129262706 ej/sec]]\n",
            "Step [100/2046]\t Loss: 4.3561201095581055 \t Time: 9.188074827194214 secs [348.2775293175526 ej/sec]]\n",
            "Step [150/2046]\t Loss: 4.255148410797119 \t Time: 9.250083208084106 secs [345.94283402806167 ej/sec]]\n",
            "Step [200/2046]\t Loss: 4.224706649780273 \t Time: 9.286119937896729 secs [344.60033053641433 ej/sec]]\n",
            "Step [250/2046]\t Loss: 4.099128723144531 \t Time: 9.18749213218689 secs [348.29961799796473 ej/sec]]\n",
            "Step [300/2046]\t Loss: 4.321977615356445 \t Time: 9.229018926620483 secs [346.7324127778973 ej/sec]]\n",
            "Step [350/2046]\t Loss: 4.2936320304870605 \t Time: 9.281005144119263 secs [344.7902409608749 ej/sec]]\n",
            "Step [400/2046]\t Loss: 4.162415504455566 \t Time: 9.205780506134033 secs [347.6076795300261 ej/sec]]\n",
            "Step [450/2046]\t Loss: 4.290327548980713 \t Time: 9.208437442779541 secs [347.5073832976042 ej/sec]]\n",
            "Step [500/2046]\t Loss: 4.09929084777832 \t Time: 9.082410097122192 secs [352.32938898166867 ej/sec]]\n",
            "Step [550/2046]\t Loss: 4.2180633544921875 \t Time: 9.137539148330688 secs [350.20369796003547 ej/sec]]\n",
            "Step [600/2046]\t Loss: 4.38916015625 \t Time: 9.139155387878418 secs [350.1417652055979 ej/sec]]\n",
            "Step [650/2046]\t Loss: 4.1703643798828125 \t Time: 9.053123474121094 secs [353.46916554793444 ej/sec]]\n",
            "Step [700/2046]\t Loss: 4.18622350692749 \t Time: 9.108783960342407 secs [351.3092432460885 ej/sec]]\n",
            "Step [750/2046]\t Loss: 4.17816686630249 \t Time: 9.237050294876099 secs [346.4309382157503 ej/sec]]\n",
            "Step [800/2046]\t Loss: 4.170524597167969 \t Time: 9.128881216049194 secs [350.53583503465705 ej/sec]]\n",
            "Step [850/2046]\t Loss: 3.9393434524536133 \t Time: 9.240965843200684 secs [346.28414976281897 ej/sec]]\n",
            "Step [900/2046]\t Loss: 4.311774253845215 \t Time: 9.096620321273804 secs [351.7790000002883 ej/sec]]\n",
            "Step [950/2046]\t Loss: 4.044496059417725 \t Time: 9.143381357192993 secs [349.9799335704834 ej/sec]]\n",
            "Step [1000/2046]\t Loss: 4.006168365478516 \t Time: 9.198967933654785 secs [347.86511085582487 ej/sec]]\n",
            "Step [1050/2046]\t Loss: 4.084226608276367 \t Time: 9.088549137115479 secs [352.0914011381596 ej/sec]]\n",
            "Step [1100/2046]\t Loss: 4.2746686935424805 \t Time: 9.281922578811646 secs [344.7561615418788 ej/sec]]\n",
            "Step [1150/2046]\t Loss: 4.014895439147949 \t Time: 9.19886040687561 secs [347.869177100262 ej/sec]]\n",
            "Step [1200/2046]\t Loss: 4.179989337921143 \t Time: 9.070386409759521 secs [352.7964361647124 ej/sec]]\n",
            "Step [1250/2046]\t Loss: 4.1130523681640625 \t Time: 9.195241689682007 secs [348.0060783601506 ej/sec]]\n",
            "Step [1300/2046]\t Loss: 4.151021480560303 \t Time: 9.102670431137085 secs [351.54518931652274 ej/sec]]\n",
            "Step [1350/2046]\t Loss: 4.036077499389648 \t Time: 9.158847332000732 secs [349.38894426368455 ej/sec]]\n",
            "Step [1400/2046]\t Loss: 4.038947105407715 \t Time: 9.386417150497437 secs [340.91815318802605 ej/sec]]\n",
            "Step [1450/2046]\t Loss: 4.0321760177612305 \t Time: 9.099222898483276 secs [351.6783834950783 ej/sec]]\n",
            "Step [1500/2046]\t Loss: 3.981370687484741 \t Time: 9.05853009223938 secs [353.25819613289167 ej/sec]]\n",
            "Step [1550/2046]\t Loss: 4.096623420715332 \t Time: 9.054320573806763 secs [353.42243229793274 ej/sec]]\n",
            "Step [1600/2046]\t Loss: 4.1025614738464355 \t Time: 9.14402723312378 secs [349.95521321373155 ej/sec]]\n",
            "Step [1650/2046]\t Loss: 4.234279632568359 \t Time: 9.085132837295532 secs [352.2237987389272 ej/sec]]\n",
            "Step [1700/2046]\t Loss: 4.014443874359131 \t Time: 9.055554628372192 secs [353.37426931024163 ej/sec]]\n",
            "Step [1750/2046]\t Loss: 4.255670070648193 \t Time: 9.117950916290283 secs [350.95604586802796 ej/sec]]\n",
            "Step [1800/2046]\t Loss: 4.0339155197143555 \t Time: 9.119187116622925 secs [350.90847013840465 ej/sec]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI_kq0N1KHLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tensorboard dev upload --logdir \"$tb_dir\" --name \"IFCB\" --description \"Training with 2006\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}