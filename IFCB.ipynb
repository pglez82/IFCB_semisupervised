{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pglez82/IFCB_semisupervised/blob/master/IFCB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RStEOrZz2isi"
   },
   "source": [
    "# Showing system info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "Mdydmkeg2pHa",
    "outputId": "f586782b-58b6-4b4e-c49b-5060260cb783"
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0aj37uariCNh"
   },
   "source": [
    "# Download SimCLR code\n",
    "In this step we download the SimCLR code for **PyTorch** and install its dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 877
    },
    "colab_type": "code",
    "id": "RU38uvtmh_HO",
    "outputId": "816d3c4a-588d-44fa-e1a5-ec3dcfa383e1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir(\"SimCLR\"):\n",
    "  !git clone https://github.com/pglez82/SimCLR.git\n",
    " \n",
    "%cd SimCLR\n",
    "!sh setup.sh || python3 -m pip install -r requirements.txt || exit 1\n",
    "!pip install  pyyaml --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ofyw_2IYzlMu"
   },
   "source": [
    "# Download the images\n",
    "In this section, we **donwload** the data and **uncompress** it. The code has checks in order to ensure that already downloaded data is not redownloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zA0THo_kbFMw",
    "outputId": "d1361d8d-fc71-456f-f7bc-f0c34bbb2088"
   },
   "outputs": [],
   "source": [
    "%cd ..\n",
    "if not os.path.isfile(\"IFCB_data.tar\") and not os.path.isdir(\"data\"):\n",
    "  print(\"Data do not exist in local. Downloading...\")\n",
    "  !wget -O IFCB_data.tar https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/Ec2z0uC4lghEg-9MjzoJ9QkBK5n74QjS-LszB9dlNrPfaw?download=1\n",
    "else:\n",
    "  print(\"Data already exists. Skipping download.\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "  print(\"Extracting the tar file...\")\n",
    "  !tar -xf \"IFCB_data.tar\"\n",
    "  print(\"Done. Removing the tar file.\")\n",
    "  !rm -f IFCB_data.tar #Remove the original file to save space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUG0AmQ6z8us"
   },
   "source": [
    "# Download CSV with information about the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "colab_type": "code",
    "id": "YoVqmVot04VX",
    "outputId": "d7d8544a-5d77-4d64-dcf1-a368e1bbadca"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not os.path.isfile('IFCB.csv.zip'):\n",
    "  print(\"CSV data do not exist. Downloading...\")\n",
    "  !wget -O IFCB.csv.zip \"https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/EfsVLhFsYJpPjO0KZlpWUq0BU6LaqJ989Re4XzatS9aG4Q?download=1\"\n",
    "\n",
    "data = pd.read_csv('IFCB.csv.zip',compression='infer', header=0,sep=',',quotechar='\"')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oB1gMsg5EIZV"
   },
   "source": [
    "# Create training set\n",
    "\n",
    "Here we make a reestructuration of the images depending on which class we consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "fX4-tijiEVcO",
    "outputId": "eebbc197-ec2d-4b57-ae36-798ed164b172"
   },
   "outputs": [],
   "source": [
    "import progressbar\n",
    "from tqdm import tqdm\n",
    "from shutil import copyfile\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "classcolumn = \"AutoClass\" #Autoclass means 51 classes\n",
    "#nclasses = 51 #Pick this number of most abundant classes to make sure that we do not have empty classes\n",
    "yearstraining = ['2012'] #Years to consider as training\n",
    "yearsvalidation = ['2013']\n",
    "trainingfolder = \"training\"\n",
    "validationfolder = \"validation\"\n",
    "\n",
    "classes=np.unique(data[classcolumn])\n",
    "#In order to ensure that all tests have same number of classes, \n",
    "#classes=list(pd.crosstab(index=data[classcolumn],columns='count').sort_values('count',ascending=False)[0:nclasses].index)\n",
    "#print(\"Considering %i classes\" % len(classes))\n",
    "#print(classes)\n",
    "\n",
    "print(\"Computing image paths...\")\n",
    "#Compute data paths\n",
    "data['year'] = data['Sample'].str[6:10].astype(str)\n",
    "data['path']=\"data\"+'/'+data['year']+'/'+data['OriginalClass'].astype(str)+'/'+data['Sample'].astype(str)+'_'+data['roi_number'].apply(lambda x: str(x).zfill(5))+'.png'\n",
    "print('Done')\n",
    "\n",
    "#data[classcolumn][~data[classcolumn].isin(classes)]='mix' #Put ignored classes into mix category\n",
    "\n",
    "#Check data by year\n",
    "print(pd.crosstab(index=data['year'],columns='count'))\n",
    "\n",
    "if not os.path.isdir(trainingfolder):\n",
    "  print(\"Create folder structure for training set... Using years:\")\n",
    "  print(yearstraining)\n",
    "  os.mkdir(trainingfolder)\n",
    "  for folder in classes:\n",
    "    os.mkdir(os.path.join(trainingfolder,folder))\n",
    "  print(\"Done.\\nMoving images to the respective folders...\")\n",
    "  data[data['year'].isin(yearstraining)].progress_apply(lambda row: copyfile(row['path'],os.path.join(trainingfolder,row[classcolumn],os.path.basename(row['path']))),axis=1)\n",
    "  print(\"Done\")\n",
    "else:\n",
    "  print(\"Training data already there... Doing nothing\")\n",
    "\n",
    "if not os.path.isdir(validationfolder):\n",
    "  print(\"Create folder structure for the validation set... Using years:\")\n",
    "  print(yearsvalidation)\n",
    "  os.mkdir(validationfolder)\n",
    "  for folder in classes:\n",
    "    os.mkdir(os.path.join(validationfolder,folder))\n",
    "  print(\"Done.\\nMoving images to the respective folders...\")\n",
    "  data[data['year'].isin(yearsvalidation)].progress_apply(lambda row: copyfile(row['path'],os.path.join(validationfolder,row[classcolumn],os.path.basename(row['path']))),axis=1)\n",
    "  print(\"Done\")  \n",
    "else:\n",
    "  print(\"Validation data already there... Doing nothing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oh9aOkEfWUEq"
   },
   "source": [
    "# Lets configure SimCLR\n",
    "Number of epocs, optimizer, resnet version to use ...\n",
    "Things that we have to configure:\n",
    "\n",
    "\n",
    "*   cuda:0 -> Change to cuda:1 to use second gpu\n",
    "*   args.batch_size -> higher value its slower but better\n",
    "*   args.resnet -> resnet18 | resnet50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dKfV68rAjVeE",
    "outputId": "0bdf18cf-9893-4d06-9b42-d60982355279"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from SimCLR.utils.yaml_config_hook import yaml_config_hook\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "torch.manual_seed(0) #Reproducible\n",
    "random.seed(0) #it seems that the transforms uses this random\n",
    "\n",
    "config = yaml_config_hook(\"./SimCLR/config/config.yaml\")\n",
    "args = argparse.Namespace(**config)\n",
    "\n",
    "#Here we need to select which graphics card we want to use in case of having more than one\n",
    "args.device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using %s\" % args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "dwtMapDQjyfY",
    "outputId": "8723201a-50c0-4d2b-c7be-446827d6cff0"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "args.dataset = \"IFCB_12_13\" #This value will be used only for the output dir\n",
    "args.image_size = 224 #@param\n",
    "args.batch_size =  256 #@param\n",
    "args.resnet = \"resnet18\" #@param ['resnet18','resnet50']\n",
    "#Means that we want to start training in this epoch. We should have a file checkpoint_{}.tar in the args.model_path dir\n",
    "args.epoch_num =  0 #@param \n",
    "#How many epochs we want. If epochs = epoch num we just load the model and do nothing\n",
    "args.epochs = 100 #@param  \n",
    "#We want to save the checkpoints to google drive\n",
    "args.out_dir = \"drive/My Drive/Colab Notebooks/{}_{}_b{}_s{}\".format(args.dataset,args.resnet,args.batch_size,args.image_size)\n",
    "args.model_path = args.out_dir #This is the directory from where we want to restore checkpoints\n",
    "args.proportions = [0.01,0.05,0.08,0.09,0.1,0.15,0.2,0.3,0.4,0.5,0.8,1] #how many labeled data we are going to use for training\n",
    "args.logistic_epochs = 500\n",
    "args.logistic_batch_size = 512\n",
    "args.workers = 4\n",
    "\n",
    "if not os.path.isdir(args.out_dir):\n",
    "  raise SystemExit(\"The output folder {} does not exist!\".format(args.out_dir))\n",
    "pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iPW_XQETJi6H"
   },
   "source": [
    "# Loading the training dataset\n",
    "\n",
    "Use pytorch to load the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zvbF4INZJpGL"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from SimCLR.modules.transformations import TransformsSimCLR\n",
    "\n",
    "#This transform makes the magic and returns two augmented images from an original image\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=trainingfolder, transform=TransformsSimCLR(size=args.image_size))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=args.batch_size,\n",
    "  shuffle=False,\n",
    "  drop_last=True,\n",
    "  num_workers=args.workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show some example pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "\n",
    "train_dataset_display = torchvision.datasets.ImageFolder(root=trainingfolder,\n",
    "                                                         transform=T.Compose([T.Resize(size=args.image_size),\n",
    "                                                                              T.CenterCrop(size=args.image_size)]))\n",
    "fig, axs = plt.subplots(nrows=4, ncols=8, constrained_layout=False, figsize=(16,8))\n",
    "fig.tight_layout()\n",
    "for _, ax in enumerate(axs.flat):\n",
    "  index = random.randint(0, len(train_dataset_display)-1)\n",
    "  ax.imshow(train_dataset_display[index][0])\n",
    "  ax.title.set_text(train_dataset_display.classes[train_dataset_display[index][1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jGzNgl0rvVvO"
   },
   "source": [
    "# Define the training function\n",
    "This is the function that will do all the work for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xd2u3DeVjL1p"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(args, train_loader, model, criterion, optimizer, writer):\n",
    "  loss_epoch = 0\n",
    "  start_time = time.time()\n",
    "  for step, ((x_i, x_j), _) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    x_i = x_i.to(args.device)\n",
    "    x_j = x_j.to(args.device)\n",
    "\n",
    "    # positive pair, with encoding\n",
    "    h_i, z_i = model(x_i)\n",
    "    h_j, z_j = model(x_j)\n",
    "\n",
    "    loss = criterion(z_i, z_j)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "      spent = time.time()-start_time\n",
    "      print(f\"Step [{step}/{len(train_loader)}]\\t Loss: {loss.item()} \\t Time: {spent} secs [{(args.batch_size*50)/spent} ej/sec]]\")\n",
    "      start_time = time.time()\n",
    "\n",
    "    writer.add_scalar(\"Loss/Step\", loss.item(), args.global_step)\n",
    "    loss_epoch += loss.item()\n",
    "    args.global_step += 1\n",
    "\n",
    "  return loss_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fPt7HTMDWHGj"
   },
   "source": [
    "# Load the model\n",
    "We only reload the model if **args.epoch_num** is different from zero. This case means that we want to continue training from a checkpoint (we should have the model in the **args.model_path** dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "asEk24AyUUyY"
   },
   "outputs": [],
   "source": [
    "from SimCLR.model import load_model\n",
    "model, optimizer, scheduler = load_model(args, train_loader,reload_model=(args.epoch_num!=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IgnQQ_UTWuNn"
   },
   "source": [
    "# Configure TensorBoard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2sVEpLMWzCK"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "tb_dir = os.path.join(args.out_dir, \"colab\")\n",
    "if not os.path.exists(tb_dir):\n",
    "  os.makedirs(tb_dir)\n",
    "writer = SummaryWriter(log_dir=tb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "esq_Jrkh_zjB"
   },
   "source": [
    "# Load the loss function\n",
    "This function tries to minimize the difference between the two augmented variations of the image and maximize the difference between these and the rest of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7XqPNffhXVCH"
   },
   "outputs": [],
   "source": [
    "from SimCLR.modules import NT_Xent\n",
    "\n",
    "criterion = NT_Xent(args.batch_size, args.temperature, args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a2RE3UTiAD49"
   },
   "source": [
    "# Training the CNN\n",
    "We make a checkpoint each 5 epochs just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ibd6L_Gbwnq"
   },
   "outputs": [],
   "source": [
    "from SimCLR.model import save_model\n",
    "\n",
    "args.global_step = 0\n",
    "if args.epoch_num!=0: #If we have loaded a model trained til an epoch, lets start training in the next\n",
    "  args.start_epoch=args.epoch_num+1\n",
    "args.current_epoch = args.start_epoch #Variable for controlling in which epoch we are\n",
    "\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "  lr = optimizer.param_groups[0]['lr']\n",
    "  loss_epoch = train(args, train_loader, model, criterion, optimizer, writer)\n",
    "\n",
    "  if scheduler:\n",
    "    scheduler.step()\n",
    "\n",
    "  if epoch % 5 == 0:\n",
    "    save_model(args, model, optimizer)\n",
    "\n",
    "  writer.add_scalar(\"Loss/train epoch\", loss_epoch / len(train_loader), epoch)\n",
    "  writer.add_scalar(\"Misc/learning_rate\", lr, epoch)\n",
    "  print(f\"Epoch [{epoch+1}/{args.epochs}]\\t Loss: {loss_epoch / len(train_loader)}\\t lr: {round(lr, 5)}\")\n",
    "  args.current_epoch += 1\n",
    "\n",
    "## end training\n",
    "if args.start_epoch!=args.epochs:\n",
    "  save_model(args, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JI_kq0N1KHLa"
   },
   "outputs": [],
   "source": [
    "#!tensorboard dev upload --logdir \"$tb_dir\" --name \"IFCBv3\" --description \"Training with 2006 image size 128 batch size 256\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OkJF3l3NqeFz"
   },
   "source": [
    "# **Trainining the classifier using the deep features**\n",
    "Now we will be trying to find out if the network has learnt something useful from the unlabeled data. We will train a Logistic Regression classifier with the labeled examples and testing against a validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h8su48ibuaBp"
   },
   "source": [
    "# Define train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvUK4Z50t0E1"
   },
   "outputs": [],
   "source": [
    "def train(args, loader, model, criterion, optimizer):\n",
    "  loss_epoch = 0\n",
    "  accuracy_epoch = 0\n",
    "  for step, (x, y) in enumerate(loader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x = x.to(args.device)\n",
    "    y = y.to(args.device)\n",
    "\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "\n",
    "    predicted = output.argmax(1)\n",
    "    acc = (predicted == y).sum().item() / y.size(0)\n",
    "    accuracy_epoch += acc\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_epoch += loss.item()\n",
    "\n",
    "  return loss_epoch, accuracy_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fS3gToslt7A4"
   },
   "outputs": [],
   "source": [
    "def make_preds(args, model, loader):\n",
    "  \"\"\"\n",
    "  Check the accuracy of the model.\n",
    "  \"\"\"\n",
    "  # Set the model to eval mode\n",
    "  model.eval()\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "  for x, y in loader:\n",
    "    x = x.to(args.device)\n",
    "    y = y.to(args.device)\n",
    "    # Run the model forward, and compare the argmax score with the ground-truth\n",
    "    # category.\n",
    "    output = model(x)\n",
    "    predicted = output.argmax(1)\n",
    "    y_true.extend(y.cpu().numpy())\n",
    "    y_pred.extend(predicted.cpu().numpy())\n",
    "  return y_true,y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T-4wy4s8uewo"
   },
   "source": [
    "# Load data\n",
    "We have to load the data again because before the data loader was doing the special agumentation for the contrastive learning. Now we only want to resize the images.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DZQ03a_BuqbR"
   },
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(root=trainingfolder, transform=TransformsSimCLR(size=args.image_size).test_transform,)\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=validationfolder, transform=TransformsSimCLR(size=args.image_size).test_transform,)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=args.logistic_batch_size,\n",
    "  num_workers=args.workers,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=args.logistic_batch_size,\n",
    "  shuffle=False,\n",
    "  num_workers=args.workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_distribution(labels,class_to_idx):\n",
    "  class_mapping = {v: k for k, v in class_to_idx.items()}\n",
    "  df = pd.DataFrame(data=labels)\n",
    "  df=df.replace(class_mapping)\n",
    "  c=pd.crosstab(index=df[0],columns='count')\n",
    "  print(c)\n",
    "\n",
    "\n",
    "labels_train = list(train_dataset.targets)\n",
    "labels_test = list(test_dataset.targets)\n",
    "print(\"Printing distribution of training set...\")\n",
    "print_class_distribution(labels_train,train_dataset.class_to_idx)\n",
    "print(\"Printing distribution of testing set...\")\n",
    "print_class_distribution(labels_test,test_dataset.class_to_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hym2t6vSygv3"
   },
   "source": [
    "# Load de pretrained CNN and its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vjDhl0Nmyk1B",
    "outputId": "7490db70-a40a-4ee3-f6c8-3434ade45132"
   },
   "outputs": [],
   "source": [
    "simclr_model, _, _ = load_model(args, train_loader, reload_model=True)\n",
    "simclr_model = simclr_model.to(args.device)\n",
    "simclr_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ecG89q-C0WIi"
   },
   "source": [
    "# Compute deep features for both training and validation sets\n",
    "Here we use our CNN pretrained using contrastive learning with unlabelled data for computing the features from all the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "7-kaL3kJ0nh9",
    "outputId": "88ef3ec2-afbd-42e8-fd69-c5e2488bc24c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_balanced_splits(y,proportions):\n",
    "  \"\"\"\n",
    "  This function creates different balanced splits following the proportions\n",
    "  \"\"\"\n",
    "  classes, globalcounts = np.unique(y, return_counts=True) #If some classes do not have examples, they are not in y\n",
    "  subsets = {}\n",
    "  for p in proportions:\n",
    "    subsets[p]=[]\n",
    "    counts = np.rint(globalcounts*p)\n",
    "    counts[counts==0]=1\n",
    "    print(\"Building subset with %d elements\"%sum(counts))\n",
    "    for i in range(len(classes)):\n",
    "      #print(\"For class %d we have %d examples and we are taking %d\" % (classes[i],globalcounts[i],counts[i]))\n",
    "      classelements, = np.where(y==classes[i])\n",
    "      subsets[p].extend(classelements[0:int(counts[i])])\n",
    "  return subsets \n",
    "\n",
    "#This function computes the deep features\n",
    "def inference(loader, context_model, device):\n",
    "  feature_vector = []\n",
    "  labels_vector = []\n",
    "  for step, (x, y) in enumerate(loader):\n",
    "    x = x.to(device)\n",
    "\n",
    "    # get encoding\n",
    "    with torch.no_grad():\n",
    "      h, z = context_model(x)\n",
    "\n",
    "    h = h.detach()\n",
    "\n",
    "    feature_vector.extend(h.cpu().detach().numpy())\n",
    "    labels_vector.extend(y.numpy())\n",
    "\n",
    "    if step % 20 == 0:\n",
    "      print(f\"Step [{step}/{len(loader)}]\")\n",
    "\n",
    "  feature_vector = np.array(feature_vector)\n",
    "  labels_vector = np.array(labels_vector)\n",
    "  print(\"Features shape {}\".format(feature_vector.shape))\n",
    "  return feature_vector, labels_vector\n",
    "\n",
    "def get_features(context_model, train_loader, test_loader, device):\n",
    "  print(\"Computing deep features for training set...\")\n",
    "  train_X, train_y = inference(train_loader, context_model, device)\n",
    "  print(\"Computing deep features for validation set...\")\n",
    "  test_X, test_y = inference(test_loader, context_model, device)\n",
    "  return train_X, train_y, test_X, test_y\n",
    "\n",
    "def create_data_loaders_from_arrays(X_train, y_train, X_test, y_test, batch_size, proportions):\n",
    "  #We want to create multiple train loaders with different labelled data proportions\n",
    "  indexes = create_balanced_splits(y_train,args.proportions)\n",
    "  train_loaders = {}\n",
    "  for p in args.proportions:\n",
    "    d = torch.utils.data.TensorDataset(torch.from_numpy(X_train[indexes[p]]), torch.from_numpy(y_train[indexes[p]]))\n",
    "    train_loaders[p]=torch.utils.data.DataLoader(d, batch_size=batch_size, shuffle=True)\n",
    "  test = torch.utils.data.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "  test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "  return train_loaders, test_loader\n",
    "\n",
    "(train_X, train_y, test_X, test_y) = get_features(simclr_model, train_loader, test_loader, args.device)\n",
    "print(\"Done\")\n",
    "\n",
    "#We create the data loaders from the arrays with the deep features\n",
    "arr_train_loaders, arr_test_loader = create_data_loaders_from_arrays(train_X, train_y, test_X, test_y, args.logistic_batch_size,args.proportions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ui_xgjsjUBXj"
   },
   "source": [
    "# Lets train the classifier and see how it works!\n",
    "We are going to train the classifier using different labeled data proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SHxNV321UF0d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from SimCLR.modules import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(35,35),facecolor='white')\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "\n",
    "for p in args.proportions:\n",
    "    model = LogisticRegression(simclr_model.n_features, len(train_dataset.classes))\n",
    "    model = model.to(args.device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    arr_train_loader = arr_train_loaders[p]\n",
    "    print(\"Training dataset with proportion %f and %d labeled examples in training set\"%(p,len(arr_train_loader)*args.logistic_batch_size))\n",
    "    for epoch in range(args.logistic_epochs):\n",
    "        loss_epoch, accuracy_epoch = train(args, arr_train_loader, model, criterion, optimizer)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{args.logistic_epochs}]\\t Loss: {loss_epoch / len(arr_train_loader)}\\t Accuracy: {accuracy_epoch / len(arr_train_loader)}\")\n",
    "\n",
    "    # final testing\n",
    "    y_true,y_pred = make_preds(args,model,arr_test_loader)\n",
    "    #Build the confusion matrix here\n",
    "    cm=confusion_matrix(y_true, y_pred)\n",
    "    #Esto no está bien. habra que ver como sacar los nombres de las labesl que existan en el train(subset) y test\n",
    "    labelswithexamples=np.union1d(np.unique(y_true),np.unique(y_pred))  \n",
    "    labelswithexamples_names = np.array(list(test_dataset.class_to_idx.keys()))[labelswithexamples]\n",
    "    plot_confusion_matrix(cm=cm,target_names=labelswithexamples_names,normalize=False)\n",
    "    print(classification_report(y_true, y_pred,target_names=labelswithexamples_names))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP5emIASnLQPb6sP3x3ZZIo",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1ExTR163G6Wr9Jq8eqjQz7sAfdJJSVlqs",
   "name": "IFCB.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
