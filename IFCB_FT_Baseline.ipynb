{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pglez82/IFCB_semisupervised/blob/master/IFCB_FT_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysuTmf6SE9KB"
   },
   "source": [
    "# Load the data\n",
    "We are going to finetune a resnet18 and extract features with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "Iv08LtzkFGee",
    "outputId": "90d7c4c1-d3fb-46b6-e4e1-612d3e424456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if not os.path.isfile(\"IFCB_data.tar\") and not os.path.isdir(\"data\"):\n",
    "  print(\"Data do not exist in local. Downloading...\")\n",
    "  !wget -O IFCB_data.tar https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/Ec2z0uC4lghEg-9MjzoJ9QkBK5n74QjS-LszB9dlNrPfaw?download=1\n",
    "else:\n",
    "  print(\"Data already exists. Skipping download.\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "  print(\"Extracting the tar file...\")\n",
    "  !tar -xf \"IFCB_data.tar\"\n",
    "  print(\"Done. Removing the tar file.\")\n",
    "  !rm -f IFCB_data.tar #Remove the original file to save space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUG0AmQ6z8us"
   },
   "source": [
    "# Download CSV with information about the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "YoVqmVot04VX",
    "outputId": "910ef3f7-1876-4e8f-91ff-7ff22bc3d551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Sample  roi_number        OriginalClass  \\\n",
      "0        IFCB1_2006_158_000036           1                  mix   \n",
      "1        IFCB1_2006_158_000036           2  Tontonia_gracillima   \n",
      "2        IFCB1_2006_158_000036           3                  mix   \n",
      "3        IFCB1_2006_158_000036           4                  mix   \n",
      "4        IFCB1_2006_158_000036           5                  mix   \n",
      "...                        ...         ...                  ...   \n",
      "3457814  IFCB5_2014_353_205141        6850       Leptocylindrus   \n",
      "3457815  IFCB5_2014_353_205141        6852                  mix   \n",
      "3457816  IFCB5_2014_353_205141        6855                  mix   \n",
      "3457817  IFCB5_2014_353_205141        6856                  mix   \n",
      "3457818  IFCB5_2014_353_205141        6857                  mix   \n",
      "\n",
      "              AutoClass FunctionalGroup  \n",
      "0                   mix      Flagellate  \n",
      "1           ciliate_mix         Ciliate  \n",
      "2                   mix      Flagellate  \n",
      "3                   mix      Flagellate  \n",
      "4                   mix      Flagellate  \n",
      "...                 ...             ...  \n",
      "3457814  Leptocylindrus          Diatom  \n",
      "3457815             mix      Flagellate  \n",
      "3457816             mix      Flagellate  \n",
      "3457817             mix      Flagellate  \n",
      "3457818             mix      Flagellate  \n",
      "\n",
      "[3457819 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not os.path.isfile('IFCB.csv.zip'):\n",
    "  print(\"CSV data do not exist. Downloading...\")\n",
    "  !wget -O IFCB.csv.zip \"https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/EfsVLhFsYJpPjO0KZlpWUq0BU6LaqJ989Re4XzatS9aG4Q?download=1\"\n",
    "\n",
    "data = pd.read_csv('IFCB.csv.zip',compression='infer', header=0,sep=',',quotechar='\"')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oB1gMsg5EIZV"
   },
   "source": [
    "# Create training set\n",
    "\n",
    "Here we make a reestructuration of the images depending on which class we consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "colab_type": "code",
    "id": "fX4-tijiEVcO",
    "outputId": "73a7f434-351d-48bc-9340-6313c69c3a35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering 51 classes\n",
      "Computing image paths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgonzalez/anaconda3/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Training data already there... Doing nothing\n",
      "Validation data already there... Doing nothing\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "classcolumn = \"AutoClass\" #Autoclass means 51 classes\n",
    "yearstraining = ['2006'] #Years to consider as training\n",
    "yearsvalidation = ['2007']\n",
    "trainingfolder = \"training\"\n",
    "validationfolder = \"validation\"\n",
    "\n",
    "classes = pd.unique(data[classcolumn])\n",
    "print(\"Considering %i classes\" % len(classes))\n",
    "\n",
    "print(\"Computing image paths...\")\n",
    "#Compute data paths\n",
    "data['year'] = data['Sample'].str[6:10].astype(str)\n",
    "data['path']=\"data\"+'/'+data['year']+'/'+data['OriginalClass'].astype(str)+'/'+data['Sample'].astype(str)+'_'+data['roi_number'].apply(lambda x: str(x).zfill(5))+'.png'\n",
    "print('Done')\n",
    "\n",
    "if not os.path.isdir(trainingfolder):\n",
    "  print(\"Create folder structure for training set...\")\n",
    "  os.mkdir(trainingfolder)\n",
    "  for folder in classes:\n",
    "    os.mkdir(os.path.join(trainingfolder,folder))\n",
    "  print(\"Done.\\nMoving images to the respective folders...\")\n",
    "  data[data['year'].isin(yearstraining)].progress_apply(lambda row: os.rename(row['path'],os.path.join(trainingfolder,row[classcolumn],os.path.basename(row['path']))),axis=1)\n",
    "  print(\"Done\")\n",
    "else:\n",
    "  print(\"Training data already there... Doing nothing\")\n",
    "\n",
    "if not os.path.isdir(validationfolder):\n",
    "  print(\"Create folder structure for the validation set...\")\n",
    "  os.mkdir(validationfolder)\n",
    "  for folder in classes:\n",
    "    os.mkdir(os.path.join(validationfolder,folder))\n",
    "  print(\"Done.\\nMoving images to the respective folders...\")\n",
    "  data[data['year'].isin(yearsvalidation)].progress_apply(lambda row: os.rename(row['path'],os.path.join(validationfolder,row[classcolumn],os.path.basename(row['path']))),axis=1)\n",
    "  print(\"Done\")  \n",
    "else:\n",
    "  print(\"Validation data already there... Doing nothing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clYlSmqOJofK"
   },
   "source": [
    "# Configure the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jW90Az7wJqhD",
    "outputId": "136a8a73-3eab-482c-af6a-6d74b924f2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_workers = 16 # @param\n",
    "batch_size = 256 # @param \n",
    "train_dir = './training'\n",
    "num_epochs_ft1 = 10 # @param\n",
    "num_epochs_ft2 = 10 # @param\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using %s\"%device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1uahB4puIqI_"
   },
   "source": [
    "# Prepare de DataLoaders for the CNN\n",
    "In this step it is important to consider that we have to use images with the same size than the original network (so we can reuse the weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbzJMEKsI2Kx"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "#Define transofrmations\n",
    "train_transform = T.Compose([\n",
    "  T.Resize(size=256),\n",
    "  T.RandomResizedCrop(size=224),\n",
    "  T.RandomHorizontalFlip(),\n",
    "  T.ToTensor(),            \n",
    "  T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "#Define data loader\n",
    "train_dset = ImageFolder(train_dir, transform=train_transform)\n",
    "train_loader = DataLoader(train_dset,batch_size=batch_size,num_workers=num_workers,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "scu2GJIaKXAM"
   },
   "source": [
    "# Load the CNN\n",
    "In this step we download a pretrained CNN with the weights from ImageNet. We change the last layer to match the number of classes that we have in our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oTq6OVVjKZjm",
    "outputId": "7fd5a702-6dc1-47a5-8d47-f25663febb5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting the CNN for 51 classes\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "print(\"Adjusting the CNN for %s classes\" % len(train_dset.classes))\n",
    "num_classes = len(train_dset.classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "#Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_W0lTw-LmHt"
   },
   "source": [
    "# Perform finetuning\n",
    "First we only update the last layer for a few epochs, then we update all the weights with a small learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "1s5zIymPLtFc",
    "outputId": "396dcce1-c13f-4739-ad01-26763d62871e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 10\n",
      "Step [0/512]\t Loss: 3.6819233894348145 \t Time: 2.361470937728882 secs [5420.350424600294 ej/sec]]\n",
      "Step [50/512]\t Loss: 1.003257393836975 \t Time: 6.679912805557251 secs [1916.1926768492003 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.7287249565124512 \t Time: 6.611286163330078 secs [1936.0831892281444 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.8700928092002869 \t Time: 6.640687704086304 secs [1927.5112112445227 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.707870602607727 \t Time: 6.644641399383545 secs [1926.364303299726 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.6451399326324463 \t Time: 6.66452693939209 secs [1920.6164393068777 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.7302033305168152 \t Time: 6.6765053272247314 secs [1917.1706413242186 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.7266544103622437 \t Time: 6.684989929199219 secs [1914.7373646878905 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.7897779941558838 \t Time: 6.69628381729126 secs [1911.507987004317 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.6071487665176392 \t Time: 6.702399730682373 secs [1909.763743484877 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.6403250694274902 \t Time: 6.6716086864471436 secs [1918.5777526194258 ej/sec]]\n",
      "Train accuracy:  0.7986442955069388\n",
      "Starting epoch 2 / 10\n",
      "Step [0/512]\t Loss: 0.8053712844848633 \t Time: 2.2674145698547363 secs [5645.1961499127365 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.6367126107215881 \t Time: 6.831724405288696 secs [1873.6118790288197 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.6887340545654297 \t Time: 6.71169900894165 secs [1907.1177034231155 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.5758391618728638 \t Time: 6.718788146972656 secs [1905.105462473528 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.6391220688819885 \t Time: 6.721197843551636 secs [1904.4224404553736 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.6491321325302124 \t Time: 6.730124473571777 secs [1901.8964731282078 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.6159562468528748 \t Time: 6.738089561462402 secs [1899.648243503304 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.5978359580039978 \t Time: 6.742076635360718 secs [1898.524845129585 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.5546873807907104 \t Time: 6.776382923126221 secs [1888.9133251777396 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.7368325591087341 \t Time: 6.7693116664886475 secs [1890.8864934327316 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.7385510206222534 \t Time: 6.744938373565674 secs [1897.7193402040457 ej/sec]]\n",
      "Train accuracy:  0.8066060060151753\n",
      "Starting epoch 3 / 10\n",
      "Step [0/512]\t Loss: 0.6256072521209717 \t Time: 2.1999034881591797 secs [5818.437067305483 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.6880682110786438 \t Time: 6.866386651992798 secs [1864.1536879204316 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.653464674949646 \t Time: 6.752538204193115 secs [1895.5834995574849 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.665221631526947 \t Time: 6.752767562866211 secs [1895.519115804875 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.7387290596961975 \t Time: 6.765769720077515 secs [1891.8763909471859 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.6569139361381531 \t Time: 6.7658185958862305 secs [1891.862724162112 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.6428731679916382 \t Time: 6.780158758163452 secs [1887.8613992022729 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.49253126978874207 \t Time: 6.780627727508545 secs [1887.7308288244865 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.6206010580062866 \t Time: 6.784335613250732 secs [1886.699115385721 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.5481656789779663 \t Time: 6.783099174499512 secs [1887.0430271933099 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.7133373022079468 \t Time: 6.745192527770996 secs [1897.64783544731 ej/sec]]\n",
      "Train accuracy:  0.8054304514434895\n",
      "Starting epoch 4 / 10\n",
      "Step [0/512]\t Loss: 0.5774672627449036 \t Time: 2.208641767501831 secs [5795.416979041346 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.5697251558303833 \t Time: 6.830186367034912 secs [1874.0337835842502 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.5684415102005005 \t Time: 6.731206893920898 secs [1901.5906362289893 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.6240586042404175 \t Time: 6.727176904678345 secs [1902.7298049941833 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.6337723731994629 \t Time: 6.730915307998657 secs [1901.6730138899786 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.6701076030731201 \t Time: 6.730854511260986 secs [1901.6901908346247 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.660187304019928 \t Time: 6.7368083000183105 secs [1900.009534183303 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.5261943340301514 \t Time: 6.749520778656006 secs [1896.4309348416868 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.5611574053764343 \t Time: 6.740851402282715 secs [1898.8699254912253 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.734614372253418 \t Time: 6.749491930007935 secs [1896.439040558265 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.6764433979988098 \t Time: 6.735985040664673 secs [1900.2417497555725 ej/sec]]\n",
      "Train accuracy:  0.809041083342239\n",
      "Starting epoch 5 / 10\n",
      "Step [0/512]\t Loss: 0.7353219389915466 \t Time: 2.200617790222168 secs [5816.548451472688 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.694827139377594 \t Time: 6.827800750732422 secs [1874.688566245425 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.5764688849449158 \t Time: 6.731671094894409 secs [1901.4595067943937 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.8069602251052856 \t Time: 6.73026967048645 secs [1901.8554421571703 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.629418671131134 \t Time: 6.7405784130096436 secs [1898.9468285533744 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.7961881160736084 \t Time: 6.752279996871948 secs [1895.6559867081505 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.4612358808517456 \t Time: 6.775017261505127 secs [1889.294079400821 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.6323301792144775 \t Time: 6.773170471191406 secs [1889.8092192486142 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.643793523311615 \t Time: 6.785485744476318 secs [1886.3793222791396 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.5158765912055969 \t Time: 6.774743318557739 secs [1889.370474736298 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.5860902667045593 \t Time: 6.748321056365967 secs [1896.768083955525 ej/sec]]\n",
      "Train accuracy:  0.8104990763499794\n",
      "Starting epoch 6 / 10\n",
      "Step [0/512]\t Loss: 0.6077468991279602 \t Time: 2.219865560531616 secs [5766.114951994949 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.5618507266044617 \t Time: 6.831511735916138 secs [1873.6702057767102 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.662812352180481 \t Time: 6.726619482040405 secs [1902.8874807286318 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.4932376742362976 \t Time: 6.729615688323975 secs [1902.0402639348738 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.5863537788391113 \t Time: 6.74118447303772 secs [1898.77610547454 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.5572252869606018 \t Time: 6.7468836307525635 secs [1897.1721909737842 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.6719821095466614 \t Time: 6.7565977573394775 secs [1894.4445799064724 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.7120754718780518 \t Time: 6.773253917694092 secs [1889.7859367950098 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.5384631156921387 \t Time: 6.778830289840698 secs [1888.231369235355 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.5387219786643982 \t Time: 6.776017665863037 secs [1889.0151459440904 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.6707556247711182 \t Time: 6.732821702957153 secs [1901.1345561665555 ej/sec]]\n",
      "Train accuracy:  0.8099418329491153\n",
      "Starting epoch 7 / 10\n",
      "Step [0/512]\t Loss: 0.6473789811134338 \t Time: 1.745147705078125 secs [7334.622715747137 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.5428087115287781 \t Time: 7.172545671463013 secs [1784.5825716978854 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.6724764704704285 \t Time: 6.71335506439209 secs [1906.6472542010663 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.6783285737037659 \t Time: 6.724747657775879 secs [1903.4171468425673 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.6351809501647949 \t Time: 6.729950666427612 secs [1901.9455913477725 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.7070211172103882 \t Time: 6.742952585220337 secs [1898.2782154001663 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.6516583561897278 \t Time: 6.739160537719727 secs [1899.3463545432958 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.6932026743888855 \t Time: 6.744493722915649 secs [1897.8444529512515 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.5765511989593506 \t Time: 6.74448037147522 secs [1897.848209943008 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.6673988103866577 \t Time: 6.754028081893921 secs [1895.1653509279913 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.6952337026596069 \t Time: 6.725836515426636 secs [1903.1089992510865 ej/sec]]\n",
      "Train accuracy:  0.8078349948855743\n",
      "Starting epoch 8 / 10\n",
      "Step [0/512]\t Loss: 0.6186140775680542 \t Time: 2.203134536743164 secs [5809.9039284826895 ej/sec]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [50/512]\t Loss: 0.4708089530467987 \t Time: 6.817950963973999 secs [1877.3968993961826 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.6480696797370911 \t Time: 6.759070873260498 secs [1893.7514105138282 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.6515858173370361 \t Time: 6.764707326889038 secs [1892.1735089885226 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.6472584009170532 \t Time: 6.768350601196289 secs [1891.1549880021923 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.5166594982147217 \t Time: 6.777374029159546 secs [1888.6370952714428 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.5609667301177979 \t Time: 6.772695302963257 secs [1889.9418071265684 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.5524332523345947 \t Time: 6.765438795089722 secs [1891.9689302769384 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.7188730835914612 \t Time: 6.759503126144409 secs [1893.630309969405 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.6563782691955566 \t Time: 6.755563497543335 secs [1894.7346146113093 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.572431206703186 \t Time: 6.736461400985718 secs [1900.1073765711817 ej/sec]]\n",
      "Train accuracy:  0.8124150776324025\n",
      "Starting epoch 9 / 10\n",
      "Step [0/512]\t Loss: 0.695872962474823 \t Time: 1.7343735694885254 secs [7380.186267353451 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.6705266833305359 \t Time: 7.2291810512542725 secs [1770.6016641787085 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.5745104551315308 \t Time: 6.76440954208374 secs [1892.2568068013559 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.606870174407959 \t Time: 6.75409722328186 secs [1895.1459502059693 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.5312833786010742 \t Time: 6.741246700286865 secs [1898.758578210068 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.6158872842788696 \t Time: 6.745190382003784 secs [1897.6484391234517 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.6845047473907471 \t Time: 6.742050886154175 secs [1898.5320959660426 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.6813560724258423 \t Time: 6.750151872634888 secs [1896.2536312540158 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.5937042236328125 \t Time: 6.759516954421997 secs [1893.6264360763812 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.5711062550544739 \t Time: 6.7863829135894775 secs [1886.129940349885 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.5858179926872253 \t Time: 6.730717897415161 secs [1901.7287895717132 ej/sec]]\n",
      "Train accuracy:  0.81381963634143\n",
      "Starting epoch 10 / 10\n",
      "Step [0/512]\t Loss: 0.557834804058075 \t Time: 2.2035820484161377 secs [5808.724031492369 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.6603962182998657 \t Time: 6.844821929931641 secs [1870.0267342276695 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.7081590890884399 \t Time: 6.755086183547974 secs [1894.8684964485615 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.5342486500740051 \t Time: 6.7553722858428955 secs [1894.7882453236093 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.7642177939414978 \t Time: 6.7616167068481445 secs [1893.0383893302026 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.5441935062408447 \t Time: 6.759040117263794 secs [1893.7600277451404 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.5602397322654724 \t Time: 6.7809929847717285 secs [1887.6291464606038 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.6106983423233032 \t Time: 6.786262035369873 secs [1886.163536463319 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.6263198852539062 \t Time: 6.7812910079956055 secs [1887.546189200246 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.5749452114105225 \t Time: 6.785623550415039 secs [1886.3410127160819 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.7887510061264038 \t Time: 6.7625463008880615 secs [1892.7781682351065 ej/sec]]\n",
      "Train accuracy:  0.8128730859070854\n",
      "Starting epoch 1 / 10\n",
      "Step [0/512]\t Loss: 0.6094212532043457 \t Time: 2.300851345062256 secs [5563.158188150422 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.6222891211509705 \t Time: 18.669511795043945 secs [685.6097867217889 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.6043661832809448 \t Time: 18.650527954101562 secs [686.3076493866795 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.5230594277381897 \t Time: 18.666340112686157 secs [685.726281784653 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.6047629117965698 \t Time: 18.69002938270569 secs [684.8571362784551 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.51995450258255 \t Time: 18.690444707870483 secs [684.8419178924064 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.5031841993331909 \t Time: 18.688642263412476 secs [684.9079681437901 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.45721080899238586 \t Time: 18.72041416168213 secs [683.7455565593027 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.4810892641544342 \t Time: 18.754655838012695 secs [682.4971948595528 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.590718686580658 \t Time: 18.742424964904785 secs [682.9425767459662 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.44242894649505615 \t Time: 18.711204767227173 secs [684.0820866018902 ej/sec]]\n",
      "Train accuracy:  0.8369643211554022\n",
      "Starting epoch 2 / 10\n",
      "Step [0/512]\t Loss: 0.45622923970222473 \t Time: 2.3055260181427 secs [5551.878356294371 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.4567928612232208 \t Time: 18.65856647491455 secs [686.0119729567069 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.5467532277107239 \t Time: 18.656539916992188 secs [686.0864906864048 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.4082939326763153 \t Time: 18.66487431526184 secs [685.7801335170917 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.5013445615768433 \t Time: 18.68188214302063 secs [685.1558050740597 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.425654798746109 \t Time: 18.67082452774048 secs [685.5615819741755 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.5994199514389038 \t Time: 18.68745255470276 secs [684.9515717849322 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.5367239117622375 \t Time: 18.665518760681152 secs [685.7564562825413 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.5527963638305664 \t Time: 18.687535047531128 secs [684.9485481869932 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.4959052503108978 \t Time: 18.69517946243286 secs [684.6684743369827 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.5216996669769287 \t Time: 18.660971641540527 secs [685.9235545648852 ej/sec]]\n",
      "Train accuracy:  0.843185600219844\n",
      "Starting epoch 3 / 10\n",
      "Step [0/512]\t Loss: 0.487761527299881 \t Time: 2.309840202331543 secs [5541.508883203147 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.45259514451026917 \t Time: 18.701619386672974 secs [684.4327079569084 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.4295996427536011 \t Time: 18.65892744064331 secs [685.9987017323805 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.4037279784679413 \t Time: 18.662962913513184 secs [685.8503689535802 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.542066752910614 \t Time: 18.674287796020508 secs [685.4344401143738 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.42207401990890503 \t Time: 18.69279670715332 secs [684.7557484590694 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.4924852252006531 \t Time: 18.68588614463806 secs [685.0089902572256 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.47462141513824463 \t Time: 18.68402123451233 secs [685.077363129752 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.39151036739349365 \t Time: 18.685898542404175 secs [685.0085357657689 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.43406036496162415 \t Time: 18.70176410675049 secs [684.4274116033674 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.4222778081893921 \t Time: 18.670564651489258 secs [685.5711243301369 ej/sec]]\n",
      "Train accuracy:  0.8492084090319232\n",
      "Starting epoch 4 / 10\n",
      "Step [0/512]\t Loss: 0.4408732056617737 \t Time: 1.8203554153442383 secs [7031.593881120988 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.4358799457550049 \t Time: 18.82477045059204 secs [679.9551704279846 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.45433932542800903 \t Time: 18.69326400756836 secs [684.7386307077059 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.45866650342941284 \t Time: 18.698162078857422 secs [684.5592602105716 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.44645532965660095 \t Time: 18.74666404724121 secs [682.7881466134061 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.4847413897514343 \t Time: 18.734745740890503 secs [683.222509503435 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.45386260747909546 \t Time: 18.724661350250244 secs [683.5904671690597 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.5114738345146179 \t Time: 18.735660314559937 secs [683.1891582733708 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.5214800238609314 \t Time: 18.710458755493164 secs [684.1093618959009 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.3650818169116974 \t Time: 18.73318600654602 secs [683.279394947941 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.5719896554946899 \t Time: 18.70538640022278 secs [684.2948724035743 ej/sec]]\n",
      "Train accuracy:  0.8528190409306728\n",
      "Starting epoch 5 / 10\n",
      "Step [0/512]\t Loss: 0.38276565074920654 \t Time: 2.262471914291382 secs [5657.528793681856 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.4237251579761505 \t Time: 18.671619653701782 secs [685.5323875163829 ej/sec]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [100/512]\t Loss: 0.4388822913169861 \t Time: 18.6731379032135 secs [685.4766492029827 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.3754936754703522 \t Time: 18.68896174430847 secs [684.896259895128 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.475953608751297 \t Time: 18.706515789031982 secs [684.253558725506 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.48445141315460205 \t Time: 18.67817258834839 secs [685.2918795698865 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.40292614698410034 \t Time: 18.697928190231323 secs [684.567823224785 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.3928326368331909 \t Time: 18.71051263809204 secs [684.1073917953992 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.4390897750854492 \t Time: 18.72868037223816 secs [683.443774232682 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.4145894944667816 \t Time: 18.7358660697937 secs [683.181655564692 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.5420541763305664 \t Time: 18.681391954421997 secs [685.1737831543202 ej/sec]]\n",
      "Train accuracy:  0.8558342620723348\n",
      "Starting epoch 6 / 10\n",
      "Step [0/512]\t Loss: 0.4273545444011688 \t Time: 2.295379638671875 secs [5576.419597154822 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.5536472797393799 \t Time: 18.6681067943573 secs [685.6613871455343 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.4246440529823303 \t Time: 18.62656307220459 secs [687.1906508131253 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.45450085401535034 \t Time: 18.641918420791626 secs [686.6246118599016 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.38667646050453186 \t Time: 18.686771154403687 secs [684.9765480744156 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.36295145750045776 \t Time: 18.677863597869873 secs [685.3032164481479 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.5007634162902832 \t Time: 18.67702579498291 secs [685.3339573712203 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.4510168135166168 \t Time: 18.709348917007446 secs [684.1499432598831 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.38718071579933167 \t Time: 18.6696138381958 secs [685.6060393607461 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.467730849981308 \t Time: 18.680858850479126 secs [685.1933362620373 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.608137309551239 \t Time: 18.652711391448975 secs [686.2273120179165 ej/sec]]\n",
      "Train accuracy:  0.856147234393368\n",
      "Starting epoch 7 / 10\n",
      "Step [0/512]\t Loss: 0.4330025911331177 \t Time: 2.3191699981689453 secs [5519.215930745045 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.45424792170524597 \t Time: 18.688453197479248 secs [684.9148971690445 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.3396647572517395 \t Time: 18.639586687088013 secs [686.7105057037985 ej/sec]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def run_epoch(model, loss_fn, loader, optimizer, device):\n",
    "  \"\"\"\n",
    "  Train the model for one epoch.\n",
    "  \"\"\"\n",
    "  start_time = time.time()\n",
    "  # Set the model to training mode\n",
    "  model.train()\n",
    "  for step, (x, y) in enumerate(loader):\n",
    "    \n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # Run the model forward to compute scores and loss.\n",
    "    scores = model(x)\n",
    "    loss = loss_fn(scores, y)\n",
    "\n",
    "    # Run the model backward and take a step using the optimizer.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50== 0:\n",
    "      spent = time.time()-start_time\n",
    "      print(f\"Step [{step}/{len(loader)}]\\t Loss: {loss.item()} \\t Time: {spent} secs [{(batch_size*50)/spent} ej/sec]]\")\n",
    "      start_time = time.time()\n",
    "\n",
    "def check_accuracy(model, loader, device):\n",
    "  \"\"\"\n",
    "  Check the accuracy of the model.\n",
    "  \"\"\"\n",
    "  # Set the model to eval mode\n",
    "  model.eval()\n",
    "  num_correct, num_examples = 0, 0\n",
    "  for x, y in loader:\n",
    "    x = x.to(device)\n",
    "\n",
    "    # Run the model forward, and compare the argmax score with the ground-truth\n",
    "    # category.\n",
    "    scores = model(x)\n",
    "    _, preds = scores.data.cpu().max(1)\n",
    "    num_correct += (preds == y).sum()\n",
    "    num_examples += x.size(0)\n",
    "\n",
    "  # Return the fraction of datapoints that were correctly classified.\n",
    "  acc = float(num_correct) / num_examples\n",
    "  return acc\n",
    "\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False\n",
    "for param in model.fc.parameters():\n",
    "  param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "\n",
    "#First phase of finetuning\n",
    "for epoch in range(num_epochs_ft1):\n",
    "  # Run an epoch over the training data.\n",
    "  print('Starting epoch %d / %d' % (epoch + 1,num_epochs_ft1))\n",
    "  run_epoch(model, loss_fn, train_loader, optimizer, device)\n",
    "\n",
    "  # Check accuracy on the train and val sets.\n",
    "  train_acc = check_accuracy(model, train_loader, device)\n",
    "  print('Train accuracy: ', train_acc)\n",
    "\n",
    "#Allow updating all the weights in the second phase\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = True\n",
    "\n",
    "#Lower learning rate this time\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Train the entire model for a few more epochs, checking accuracy on the\n",
    "# train sets after each epoch.\n",
    "for epoch in range(num_epochs_ft2):\n",
    "  print('Starting epoch %d / %d' % (epoch + 1, num_epochs_ft2))\n",
    "  run_epoch(model, loss_fn, train_loader, optimizer, device)\n",
    "\n",
    "  train_acc = check_accuracy(model, train_loader, device)\n",
    "  print('Train accuracy: ', train_acc)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNshTTuV/wHM/CMmYWttHqR",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "IFCB_FT_Baseline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
