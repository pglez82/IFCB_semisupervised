{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pglez82/IFCB_semisupervised/blob/master/IFCB_FT_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysuTmf6SE9KB"
   },
   "source": [
    "# Load the data\n",
    "We are going to finetune a resnet18 and extract features with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "Iv08LtzkFGee",
    "outputId": "90d7c4c1-d3fb-46b6-e4e1-612d3e424456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if not os.path.isfile(\"IFCB_data.tar\") and not os.path.isdir(\"data\"):\n",
    "  print(\"Data do not exist in local. Downloading...\")\n",
    "  !wget -O IFCB_data.tar https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/Ec2z0uC4lghEg-9MjzoJ9QkBK5n74QjS-LszB9dlNrPfaw?download=1\n",
    "else:\n",
    "  print(\"Data already exists. Skipping download.\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "  print(\"Extracting the tar file...\")\n",
    "  !tar -xf \"IFCB_data.tar\"\n",
    "  print(\"Done. Removing the tar file.\")\n",
    "  !rm -f IFCB_data.tar #Remove the original file to save space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUG0AmQ6z8us"
   },
   "source": [
    "# Download CSV with information about the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "YoVqmVot04VX",
    "outputId": "910ef3f7-1876-4e8f-91ff-7ff22bc3d551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Sample  roi_number        OriginalClass  \\\n",
      "0        IFCB1_2006_158_000036           1                  mix   \n",
      "1        IFCB1_2006_158_000036           2  Tontonia_gracillima   \n",
      "2        IFCB1_2006_158_000036           3                  mix   \n",
      "3        IFCB1_2006_158_000036           4                  mix   \n",
      "4        IFCB1_2006_158_000036           5                  mix   \n",
      "...                        ...         ...                  ...   \n",
      "3457814  IFCB5_2014_353_205141        6850       Leptocylindrus   \n",
      "3457815  IFCB5_2014_353_205141        6852                  mix   \n",
      "3457816  IFCB5_2014_353_205141        6855                  mix   \n",
      "3457817  IFCB5_2014_353_205141        6856                  mix   \n",
      "3457818  IFCB5_2014_353_205141        6857                  mix   \n",
      "\n",
      "              AutoClass FunctionalGroup  \n",
      "0                   mix      Flagellate  \n",
      "1           ciliate_mix         Ciliate  \n",
      "2                   mix      Flagellate  \n",
      "3                   mix      Flagellate  \n",
      "4                   mix      Flagellate  \n",
      "...                 ...             ...  \n",
      "3457814  Leptocylindrus          Diatom  \n",
      "3457815             mix      Flagellate  \n",
      "3457816             mix      Flagellate  \n",
      "3457817             mix      Flagellate  \n",
      "3457818             mix      Flagellate  \n",
      "\n",
      "[3457819 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not os.path.isfile('IFCB.csv.zip'):\n",
    "  print(\"CSV data do not exist. Downloading...\")\n",
    "  !wget -O IFCB.csv.zip \"https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/EfsVLhFsYJpPjO0KZlpWUq0BU6LaqJ989Re4XzatS9aG4Q?download=1\"\n",
    "\n",
    "data = pd.read_csv('IFCB.csv.zip',compression='infer', header=0,sep=',',quotechar='\"')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oB1gMsg5EIZV"
   },
   "source": [
    "# Create training set\n",
    "\n",
    "Here we make a reestructuration of the images depending on which class we consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "colab_type": "code",
    "id": "fX4-tijiEVcO",
    "outputId": "73a7f434-351d-48bc-9340-6313c69c3a35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing image paths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgonzalez/anaconda3/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "col_0   count\n",
      "year         \n",
      "2006   131002\n",
      "2007   273080\n",
      "2008   427308\n",
      "2009   732398\n",
      "2010   327996\n",
      "2011   419692\n",
      "2012   394766\n",
      "2013   422255\n",
      "2014   329322\n",
      "Training data already there... Doing nothing\n",
      "Validation data already there... Doing nothing\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "from tqdm import tqdm\n",
    "from shutil import copyfile\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "classcolumn = \"AutoClass\" #Autoclass means 51 classes\n",
    "#nclasses = 51 #Pick this number of most abundant classes to make sure that we do not have empty classes\n",
    "yearstraining = ['2013'] #Years to consider as training\n",
    "yearsvalidation = ['2014']\n",
    "trainingfolder = \"training\"\n",
    "validationfolder = \"validation\"\n",
    "\n",
    "#In order to ensure that all tests have same number of classes, \n",
    "#classes=list(pd.crosstab(index=data[classcolumn],columns='count').sort_values('count',ascending=False)[0:nclasses].index)\n",
    "#print(\"Considering %i classes\" % len(classes))\n",
    "#print(classes)\n",
    "\n",
    "print(\"Computing image paths...\")\n",
    "#Compute data paths\n",
    "data['year'] = data['Sample'].str[6:10].astype(str)\n",
    "data['path']=\"data\"+'/'+data['year']+'/'+data['OriginalClass'].astype(str)+'/'+data['Sample'].astype(str)+'_'+data['roi_number'].apply(lambda x: str(x).zfill(5))+'.png'\n",
    "print('Done')\n",
    "\n",
    "#data[classcolumn][~data[classcolumn].isin(classes)]='mix' #Put ignored classes into mix category\n",
    "\n",
    "#Check data by year\n",
    "print(pd.crosstab(index=data['year'],columns='count'))\n",
    "\n",
    "if not os.path.isdir(trainingfolder):\n",
    "  print(\"Create folder structure for training set... Using years:\")\n",
    "  print(yearstraining)\n",
    "  os.mkdir(trainingfolder)\n",
    "  for folder in classes:\n",
    "    os.mkdir(os.path.join(trainingfolder,folder))\n",
    "  print(\"Done.\\nMoving images to the respective folders...\")\n",
    "  data[data['year'].isin(yearstraining)].progress_apply(lambda row: copyfile(row['path'],os.path.join(trainingfolder,row[classcolumn],os.path.basename(row['path']))),axis=1)\n",
    "  print(\"Done\")\n",
    "else:\n",
    "  print(\"Training data already there... Doing nothing\")\n",
    "\n",
    "if not os.path.isdir(validationfolder):\n",
    "  print(\"Create folder structure for the validation set... Using years:\")\n",
    "  print(yearsvalidation)\n",
    "  os.mkdir(validationfolder)\n",
    "  for folder in classes:\n",
    "    os.mkdir(os.path.join(validationfolder,folder))\n",
    "  print(\"Done.\\nMoving images to the respective folders...\")\n",
    "  data[data['year'].isin(yearsvalidation)].progress_apply(lambda row: copyfile(row['path'],os.path.join(validationfolder,row[classcolumn],os.path.basename(row['path']))),axis=1)\n",
    "  print(\"Done\")  \n",
    "else:\n",
    "  print(\"Validation data already there... Doing nothing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clYlSmqOJofK"
   },
   "source": [
    "# Configure the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jW90Az7wJqhD",
    "outputId": "136a8a73-3eab-482c-af6a-6d74b924f2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "torch.manual_seed(0) #Reproducible\n",
    "random.seed(0) #it seems that the transforms uses this random\n",
    "\n",
    "num_workers = 4 # @param\n",
    "batch_size = 256 # @param \n",
    "train_dir = './training'\n",
    "val_dir = './validation'\n",
    "num_epochs_ft1 = 10 # @param\n",
    "num_epochs_ft2 = 10 # @param\n",
    "proportion = 0.1 #How many labelled examples do we take\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using %s\"%device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1uahB4puIqI_"
   },
   "source": [
    "# Prepare de DataLoaders for the CNN\n",
    "In this step it is important to consider that we have to use images with the same size than the original network (so we can reuse the weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbzJMEKsI2Kx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building subset with 42228 elements\n",
      "Working with 0.100000 of the current data\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def create_balanced_splits(train_dset,proportions):\n",
    "  \"\"\"\n",
    "  This function creates different balanced splits following the proportions\n",
    "  \"\"\"\n",
    "  y = np.array(train_dset.targets)\n",
    "  classes, globalcounts = np.unique(y, return_counts=True) #If some classes do not have examples, they are not in y\n",
    "  subsets = {}\n",
    "  for p in proportions:\n",
    "    subsets[p]=[]\n",
    "    counts = np.rint(globalcounts*p)\n",
    "    counts[counts==0]=1\n",
    "    print(\"Building subset with %d elements\"%sum(counts))\n",
    "    for i in range(len(classes)):\n",
    "      #print(\"For class %d we have %d examples and we are taking %d\" % (classes[i],globalcounts[i],counts[i]))\n",
    "      classelements, = np.where(y==classes[i])\n",
    "      subsets[p].extend(classelements[0:int(counts[i])])\n",
    "  return subsets\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "#Define transofrmations\n",
    "train_transform = T.Compose([\n",
    "  T.Resize(size=256),\n",
    "  T.RandomResizedCrop(size=224),\n",
    "  T.RandomHorizontalFlip(),\n",
    "  T.ToTensor(),            \n",
    "  #T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "  T.Resize(size=256),\n",
    "  T.CenterCrop(size=224),\n",
    "  T.ToTensor(),\n",
    "  #T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "  \n",
    "\n",
    "#Define data loader\n",
    "full_train_dset = ImageFolder(train_dir, transform=train_transform)\n",
    "train_classes = full_train_dset.classes\n",
    "num_classes = len(full_train_dset.classes)\n",
    "indexes=create_balanced_splits(full_train_dset,[proportion])[proportion] #We only pass one value, this function if prepared to receive multiple proportions\n",
    "train_dset=torch.utils.data.Subset(full_train_dset,indexes)\n",
    "train_loader = DataLoader(train_dset,batch_size=batch_size,num_workers=num_workers,shuffle=True)\n",
    "print(\"Working with %f of the current data\"%proportion)\n",
    "val_dset = ImageFolder(val_dir, transform=val_transform)\n",
    "val_loader = DataLoader(val_dset,batch_size=batch_size,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "scu2GJIaKXAM"
   },
   "source": [
    "# Load the CNN\n",
    "In this step we download a pretrained CNN with the weights from ImageNet. We change the last layer to match the number of classes that we have in our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oTq6OVVjKZjm",
    "outputId": "7fd5a702-6dc1-47a5-8d47-f25663febb5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting the CNN for 51 classes\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "print(\"Adjusting the CNN for %s classes\" % num_classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "#Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define finetuning util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_epoch(model, loss_fn, loader, optimizer, device):\n",
    "  \"\"\"\n",
    "  Train the model for one epoch.\n",
    "  \"\"\"\n",
    "  loss_epoch = 0\n",
    "  start_time = time.time()\n",
    "  # Set the model to training mode\n",
    "  model.train()\n",
    "  for step, (x, y) in enumerate(loader):\n",
    "    \n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # Run the model forward to compute scores and loss.\n",
    "    scores = model(x)\n",
    "    loss = loss_fn(scores, y)\n",
    "    loss_epoch = loss_epoch + loss.item()\n",
    "    # Run the model backward and take a step using the optimizer.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50== 0:\n",
    "      spent = time.time()-start_time\n",
    "      print(f\"Step [{step}/{len(loader)}]\\t Loss: {loss.item()} \\t Time: {spent} secs [{(batch_size*50)/spent} ej/sec]]\")\n",
    "      start_time = time.time()\n",
    "  return loss_epoch\n",
    "\n",
    "def make_preds(model, loader, device):\n",
    "  \"\"\"\n",
    "  Check the accuracy of the model.\n",
    "  \"\"\"\n",
    "  # Set the model to eval mode\n",
    "  model.eval()\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "  for x, y in loader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    # Run the model forward, and compare the argmax score with the ground-truth\n",
    "    # category.\n",
    "    output = model(x)\n",
    "    predicted = output.argmax(1)\n",
    "    y_true.extend(y.cpu().numpy())\n",
    "    y_pred.extend(predicted.cpu().numpy())\n",
    "  return y_true,y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_W0lTw-LmHt"
   },
   "source": [
    "# Perform finetuning\n",
    "First we only update the last layer for a few epochs, then we update all the weights with a small learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "1s5zIymPLtFc",
    "outputId": "396dcce1-c13f-4739-ad01-26763d62871e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 10\n",
      "Step [0/165]\t Loss: 4.733484745025635 \t Time: 1.0734689235687256 secs [11923.959528745983 ej/sec]]\n",
      "Step [50/165]\t Loss: 0.9951155185699463 \t Time: 8.154690980911255 secs [1569.6486880940827 ej/sec]]\n",
      "Step [100/165]\t Loss: 0.7774373292922974 \t Time: 8.220768690109253 secs [1557.0320103277215 ej/sec]]\n",
      "Step [150/165]\t Loss: 0.719306230545044 \t Time: 8.172091960906982 secs [1566.3064073717776 ej/sec]]\n",
      "Epoch [1/10]\t Loss: 0.9942117253939311\n",
      "Starting epoch 2 / 10\n",
      "Step [0/165]\t Loss: 0.7644696831703186 \t Time: 1.0322885513305664 secs [12399.633787957315 ej/sec]]\n",
      "Step [50/165]\t Loss: 0.6066768765449524 \t Time: 8.21817684173584 secs [1557.5230670378699 ej/sec]]\n",
      "Step [100/165]\t Loss: 0.7127817869186401 \t Time: 8.513949871063232 secs [1503.4150064124726 ej/sec]]\n",
      "Step [150/165]\t Loss: 0.6417328119277954 \t Time: 8.4175124168396 secs [1520.6392775130387 ej/sec]]\n",
      "Epoch [2/10]\t Loss: 0.6637911410042734\n",
      "Starting epoch 3 / 10\n",
      "Step [0/165]\t Loss: 0.5663544535636902 \t Time: 1.0590770244598389 secs [12085.995356691252 ej/sec]]\n",
      "Step [50/165]\t Loss: 0.6140083074569702 \t Time: 8.063735485076904 secs [1587.353655596495 ej/sec]]\n",
      "Step [100/165]\t Loss: 0.5109906196594238 \t Time: 8.089637517929077 secs [1582.2711427590345 ej/sec]]\n",
      "Step [150/165]\t Loss: 0.5360562801361084 \t Time: 8.077391147613525 secs [1584.6700705811152 ej/sec]]\n",
      "Epoch [3/10]\t Loss: 0.6142857959776213\n",
      "Starting epoch 4 / 10\n",
      "Step [0/165]\t Loss: 0.5342398285865784 \t Time: 1.0423290729522705 secs [12280.190903382896 ej/sec]]\n",
      "Step [50/165]\t Loss: 0.4986085593700409 \t Time: 8.136051416397095 secs [1573.2447282969915 ej/sec]]\n",
      "Step [100/165]\t Loss: 0.585820734500885 \t Time: 8.138080835342407 secs [1572.8524032855032 ej/sec]]\n",
      "Step [150/165]\t Loss: 0.46392303705215454 \t Time: 8.038295984268188 secs [1592.3772930296395 ej/sec]]\n",
      "Epoch [4/10]\t Loss: 0.5826478483098926\n",
      "Starting epoch 5 / 10\n",
      "Step [0/165]\t Loss: 0.5227642059326172 \t Time: 1.0303971767425537 secs [12422.394285342747 ej/sec]]\n",
      "Step [50/165]\t Loss: 0.609447181224823 \t Time: 8.07681393623352 secs [1584.7833193950057 ej/sec]]\n",
      "Step [100/165]\t Loss: 0.5420600771903992 \t Time: 8.093183279037476 secs [1581.5779228865192 ej/sec]]\n",
      "Step [150/165]\t Loss: 0.48374736309051514 \t Time: 8.167486906051636 secs [1567.1895342147338 ej/sec]]\n",
      "Epoch [5/10]\t Loss: 0.5683179255687829\n",
      "Starting epoch 6 / 10\n",
      "Step [0/165]\t Loss: 0.5065218210220337 \t Time: 1.0665295124053955 secs [12001.543183865153 ej/sec]]\n",
      "Step [50/165]\t Loss: 0.605907678604126 \t Time: 8.304173946380615 secs [1541.3935308494947 ej/sec]]\n",
      "Step [100/165]\t Loss: 0.5221369862556458 \t Time: 8.40447211265564 secs [1522.998687892066 ej/sec]]\n",
      "Step [150/165]\t Loss: 0.5285038948059082 \t Time: 8.377700328826904 secs [1527.8655833458695 ej/sec]]\n",
      "Epoch [6/10]\t Loss: 0.5562395321600365\n",
      "Starting epoch 7 / 10\n",
      "Step [0/165]\t Loss: 0.5771509408950806 \t Time: 1.0611674785614014 secs [12062.18646782565 ej/sec]]\n",
      "Step [50/165]\t Loss: 0.4817790985107422 \t Time: 8.16200041770935 secs [1568.2429974185538 ej/sec]]\n",
      "Step [100/165]\t Loss: 0.5744300484657288 \t Time: 8.20756721496582 secs [1559.5364210554692 ej/sec]]\n",
      "Step [150/165]\t Loss: 0.6037704348564148 \t Time: 8.152818441390991 secs [1570.0092050395435 ej/sec]]\n",
      "Epoch [7/10]\t Loss: 0.5474923388524489\n",
      "Starting epoch 8 / 10\n",
      "Step [0/165]\t Loss: 0.5274013876914978 \t Time: 1.058511734008789 secs [12092.449794131162 ej/sec]]\n",
      "Step [50/165]\t Loss: 0.6283332705497742 \t Time: 8.212583303451538 secs [1558.5838860983595 ej/sec]]\n",
      "Step [100/165]\t Loss: 0.4310184717178345 \t Time: 8.132169485092163 secs [1573.995724445349 ej/sec]]\n",
      "Step [150/165]\t Loss: 0.611226499080658 \t Time: 8.078473806381226 secs [1584.4576966863738 ej/sec]]\n",
      "Epoch [8/10]\t Loss: 0.5391685964483204\n",
      "Starting epoch 9 / 10\n",
      "Step [0/165]\t Loss: 0.5205026268959045 \t Time: 1.0289385318756104 secs [12440.004532309038 ej/sec]]\n",
      "Step [50/165]\t Loss: 0.544182538986206 \t Time: 8.179168224334717 secs [1564.9513066521058 ej/sec]]\n",
      "Step [100/165]\t Loss: 0.5775222182273865 \t Time: 8.385034799575806 secs [1526.5291445955054 ej/sec]]\n",
      "Step [150/165]\t Loss: 0.5069590210914612 \t Time: 8.33413052558899 secs [1535.8530755786787 ej/sec]]\n",
      "Epoch [9/10]\t Loss: 0.5396489076542131\n",
      "Starting epoch 10 / 10\n",
      "Step [0/165]\t Loss: 0.5486239790916443 \t Time: 1.0349817276000977 secs [12367.368098063407 ej/sec]]\n",
      "Step [50/165]\t Loss: 0.6629343032836914 \t Time: 8.255587339401245 secs [1550.4650939745673 ej/sec]]\n",
      "Step [100/165]\t Loss: 0.4920060634613037 \t Time: 8.290772438049316 secs [1543.8850958272872 ej/sec]]\n",
      "Step [150/165]\t Loss: 0.4738014340400696 \t Time: 8.50324821472168 secs [1505.3071105038837 ej/sec]]\n",
      "Epoch [10/10]\t Loss: 0.5295939239588651\n",
      "Starting epoch 1 / 10\n",
      "Step [0/165]\t Loss: 0.6826496124267578 \t Time: 1.1004831790924072 secs [11631.254564523597 ej/sec]]\n",
      "Step [50/165]\t Loss: 0.5529603958129883 \t Time: 18.721264839172363 secs [683.7144877742068 ej/sec]]\n",
      "Step [100/165]\t Loss: 0.400418221950531 \t Time: 18.718569040298462 secs [683.8129545289167 ej/sec]]\n",
      "Step [150/165]\t Loss: 0.48885461688041687 \t Time: 18.849095582962036 secs [679.0776747702474 ej/sec]]\n",
      "Epoch [1/10]\t Loss: 0.49087581092661076\n",
      "Starting epoch 2 / 10\n",
      "Step [0/165]\t Loss: 0.5692892074584961 \t Time: 1.061579704284668 secs [12057.50255806286 ej/sec]]\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False\n",
    "for param in model.fc.parameters():\n",
    "  param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "\n",
    "#First phase of finetuning\n",
    "for epoch in range(num_epochs_ft1):\n",
    "  # Run an epoch over the training data.\n",
    "  print('Starting epoch %d / %d' % (epoch + 1,num_epochs_ft1))\n",
    "  loss_epoch = run_epoch(model, loss_fn, train_loader, optimizer, device)\n",
    "\n",
    "  # Check accuracy on the train and val sets.\n",
    "  #train_acc = check_accuracy(model, train_loader, device)\n",
    "  print(f\"Epoch [{epoch+1}/{num_epochs_ft1}]\\t Loss: {loss_epoch / len(train_loader)}\")\n",
    "\n",
    "#Allow updating all the weights in the second phase\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = True\n",
    "\n",
    "#Lower learning rate this time\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Train the entire model for a few more epochs, checking accuracy on the\n",
    "# train sets after each epoch.\n",
    "for epoch in range(num_epochs_ft2):\n",
    "  print('Starting epoch %d / %d' % (epoch + 1, num_epochs_ft2))\n",
    "  loss_epoch = run_epoch(model, loss_fn, train_loader, optimizer, device)\n",
    "\n",
    "  print(f\"Epoch [{epoch+1}/{num_epochs_ft2}]\\t Loss: {loss_epoch / len(train_loader)}\")\n",
    "    \n",
    "print(\"Performing final validation in test examples...\")\n",
    "y_true,y_pred = make_preds(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(40,40))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "\n",
    "cm=confusion_matrix(y_true, y_pred)\n",
    "labelswithexamples=np.union1d(np.unique(val_dset.targets),np.unique(full_train_dset.targets))\n",
    "labelswithexamples_names = np.array(list(val_dset.class_to_idx.keys()))[labelswithexamples]\n",
    "plot_confusion_matrix(cm=cm,target_names=labelswithexamples_names,normalize=False)\n",
    "print(classification_report(y_true, y_pred,target_names=labelswithexamples_names))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNshTTuV/wHM/CMmYWttHqR",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "IFCB_FT_Baseline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
