{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pglez82/IFCB_semisupervised/blob/master/IFCB_FT_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysuTmf6SE9KB"
   },
   "source": [
    "# Load the data\n",
    "We are going to finetune a resnet18 and extract features with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "Iv08LtzkFGee",
    "outputId": "90d7c4c1-d3fb-46b6-e4e1-612d3e424456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if not os.path.isfile(\"IFCB_data.tar\") and not os.path.isdir(\"data\"):\n",
    "  print(\"Data do not exist in local. Downloading...\")\n",
    "  !wget -O IFCB_data.tar https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/Ec2z0uC4lghEg-9MjzoJ9QkBK5n74QjS-LszB9dlNrPfaw?download=1\n",
    "else:\n",
    "  print(\"Data already exists. Skipping download.\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "  print(\"Extracting the tar file...\")\n",
    "  !tar -xf \"IFCB_data.tar\"\n",
    "  print(\"Done. Removing the tar file.\")\n",
    "  !rm -f IFCB_data.tar #Remove the original file to save space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUG0AmQ6z8us"
   },
   "source": [
    "# Download CSV with information about the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "YoVqmVot04VX",
    "outputId": "910ef3f7-1876-4e8f-91ff-7ff22bc3d551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Sample  roi_number        OriginalClass  \\\n",
      "0        IFCB1_2006_158_000036           1                  mix   \n",
      "1        IFCB1_2006_158_000036           2  Tontonia_gracillima   \n",
      "2        IFCB1_2006_158_000036           3                  mix   \n",
      "3        IFCB1_2006_158_000036           4                  mix   \n",
      "4        IFCB1_2006_158_000036           5                  mix   \n",
      "...                        ...         ...                  ...   \n",
      "3457814  IFCB5_2014_353_205141        6850       Leptocylindrus   \n",
      "3457815  IFCB5_2014_353_205141        6852                  mix   \n",
      "3457816  IFCB5_2014_353_205141        6855                  mix   \n",
      "3457817  IFCB5_2014_353_205141        6856                  mix   \n",
      "3457818  IFCB5_2014_353_205141        6857                  mix   \n",
      "\n",
      "              AutoClass FunctionalGroup  \n",
      "0                   mix      Flagellate  \n",
      "1           ciliate_mix         Ciliate  \n",
      "2                   mix      Flagellate  \n",
      "3                   mix      Flagellate  \n",
      "4                   mix      Flagellate  \n",
      "...                 ...             ...  \n",
      "3457814  Leptocylindrus          Diatom  \n",
      "3457815             mix      Flagellate  \n",
      "3457816             mix      Flagellate  \n",
      "3457817             mix      Flagellate  \n",
      "3457818             mix      Flagellate  \n",
      "\n",
      "[3457819 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not os.path.isfile('IFCB.csv.zip'):\n",
    "  print(\"CSV data do not exist. Downloading...\")\n",
    "  !wget -O IFCB.csv.zip \"https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/EfsVLhFsYJpPjO0KZlpWUq0BU6LaqJ989Re4XzatS9aG4Q?download=1\"\n",
    "\n",
    "data = pd.read_csv('IFCB.csv.zip',compression='infer', header=0,sep=',',quotechar='\"')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oB1gMsg5EIZV"
   },
   "source": [
    "# Create training set\n",
    "\n",
    "Here we make a reestructuration of the images depending on which class we consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "colab_type": "code",
    "id": "fX4-tijiEVcO",
    "outputId": "73a7f434-351d-48bc-9340-6313c69c3a35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering 51 classes\n",
      "Computing image paths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgonzalez/anaconda3/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Training data already there... Doing nothing\n",
      "Validation data already there... Doing nothing\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "classcolumn = \"AutoClass\" #Autoclass means 51 classes\n",
    "yearstraining = ['2006'] #Years to consider as training\n",
    "yearsvalidation = ['2007']\n",
    "trainingfolder = \"training\"\n",
    "validationfolder = \"validation\"\n",
    "\n",
    "classes = pd.unique(data[classcolumn])\n",
    "print(\"Considering %i classes\" % len(classes))\n",
    "\n",
    "print(\"Computing image paths...\")\n",
    "#Compute data paths\n",
    "data['year'] = data['Sample'].str[6:10].astype(str)\n",
    "data['path']=\"data\"+'/'+data['year']+'/'+data['OriginalClass'].astype(str)+'/'+data['Sample'].astype(str)+'_'+data['roi_number'].apply(lambda x: str(x).zfill(5))+'.png'\n",
    "print('Done')\n",
    "\n",
    "if not os.path.isdir(trainingfolder):\n",
    "  print(\"Create folder structure for training set...\")\n",
    "  os.mkdir(trainingfolder)\n",
    "  for folder in classes:\n",
    "    os.mkdir(os.path.join(trainingfolder,folder))\n",
    "  print(\"Done.\\nMoving images to the respective folders...\")\n",
    "  data[data['year'].isin(yearstraining)].progress_apply(lambda row: os.rename(row['path'],os.path.join(trainingfolder,row[classcolumn],os.path.basename(row['path']))),axis=1)\n",
    "  print(\"Done\")\n",
    "else:\n",
    "  print(\"Training data already there... Doing nothing\")\n",
    "\n",
    "if not os.path.isdir(validationfolder):\n",
    "  print(\"Create folder structure for the validation set...\")\n",
    "  os.mkdir(validationfolder)\n",
    "  for folder in classes:\n",
    "    os.mkdir(os.path.join(validationfolder,folder))\n",
    "  print(\"Done.\\nMoving images to the respective folders...\")\n",
    "  data[data['year'].isin(yearsvalidation)].progress_apply(lambda row: os.rename(row['path'],os.path.join(validationfolder,row[classcolumn],os.path.basename(row['path']))),axis=1)\n",
    "  print(\"Done\")  \n",
    "else:\n",
    "  print(\"Validation data already there... Doing nothing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clYlSmqOJofK"
   },
   "source": [
    "# Configure the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jW90Az7wJqhD",
    "outputId": "136a8a73-3eab-482c-af6a-6d74b924f2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_workers = 16 # @param\n",
    "batch_size = 256 # @param \n",
    "train_dir = './training'\n",
    "val_dir = './validation'\n",
    "num_epochs_ft1 = 10 # @param\n",
    "num_epochs_ft2 = 10 # @param\n",
    "proportion = 1 #How many labelled examples do we take\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using %s\"%device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1uahB4puIqI_"
   },
   "source": [
    "# Prepare de DataLoaders for the CNN\n",
    "In this step it is important to consider that we have to use images with the same size than the original network (so we can reuse the weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbzJMEKsI2Kx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building subset with 131002 elements\n",
      "Working with 1.000000 of the current data\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "def create_balanced_splits(train_loader,proportions):\n",
    "  \"\"\"\n",
    "  This function creates different balanced splits following the proportions\n",
    "  \"\"\"\n",
    "  labels_vector = []\n",
    "  for x,y in train_loader:\n",
    "    labels_vector.extend(y.numpy())\n",
    "  unique, globalcounts = np.unique(labels_vector, return_counts=True)\n",
    "  #Find indexes for each class\n",
    "  classindexes = []\n",
    "  for c in unique:\n",
    "    classindexes.append(np.where(labels_vector == c)[0])\n",
    "  subsets = {}\n",
    "  for p in proportions:\n",
    "    subsets[p]=[]\n",
    "    counts = np.rint(globalcounts*p)\n",
    "    print(\"Building subset with %d elements\"%sum(counts))\n",
    "    for i in range(len(counts)):\n",
    "      subsets[p].extend(classindexes[i][0:int(counts[i])])\n",
    "  return subsets\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "#Define transofrmations\n",
    "train_transform = T.Compose([\n",
    "  T.Resize(size=256),\n",
    "  T.RandomResizedCrop(size=224),\n",
    "  T.RandomHorizontalFlip(),\n",
    "  T.ToTensor(),            \n",
    "  #T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "  T.Resize(size=256),\n",
    "  T.CenterCrop(size=224),\n",
    "  T.ToTensor(),\n",
    "  #T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "  \n",
    "\n",
    "#Define data loader\n",
    "train_dset = ImageFolder(train_dir, transform=train_transform)\n",
    "train_loader = DataLoader(train_dset,batch_size=batch_size,num_workers=num_workers,shuffle=False)\n",
    "num_classes = len(train_dset.classes)\n",
    "indexes=create_balanced_splits(train_loader,[proportion])[proportion] #We only pass one value, this function if prepared to receive multiple proportions\n",
    "train_dset=torch.utils.data.Subset(train_dset,indexes)\n",
    "train_loader = DataLoader(train_dset,batch_size=batch_size,num_workers=num_workers,shuffle=True)\n",
    "print(\"Working with %f of the current data\"%proportion)\n",
    "\n",
    "val_dset = ImageFolder(val_dir, transform=val_transform)\n",
    "val_loader = DataLoader(val_dset,batch_size=batch_size,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "scu2GJIaKXAM"
   },
   "source": [
    "# Load the CNN\n",
    "In this step we download a pretrained CNN with the weights from ImageNet. We change the last layer to match the number of classes that we have in our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oTq6OVVjKZjm",
    "outputId": "7fd5a702-6dc1-47a5-8d47-f25663febb5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting the CNN for 51 classes\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "print(\"Adjusting the CNN for %s classes\" % num_classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "#Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_W0lTw-LmHt"
   },
   "source": [
    "# Perform finetuning\n",
    "First we only update the last layer for a few epochs, then we update all the weights with a small learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "1s5zIymPLtFc",
    "outputId": "396dcce1-c13f-4739-ad01-26763d62871e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 10\n",
      "Step [0/512]\t Loss: 0.7009488344192505 \t Time: 2.3488380908966064 secs [5449.502905121034 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.6747325658798218 \t Time: 7.595005750656128 secs [1685.3180129447849 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.6512468457221985 \t Time: 7.529764175415039 secs [1699.9204359935304 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.6653121709823608 \t Time: 7.5224809646606445 secs [1701.5662864595147 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.6299343705177307 \t Time: 7.591899156570435 secs [1686.0076426228866 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.6107427477836609 \t Time: 7.587847948074341 secs [1686.9078146522968 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.5977334976196289 \t Time: 7.549274206161499 secs [1695.5272322143248 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.5632631182670593 \t Time: 7.542078256607056 secs [1697.144946591726 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.573750376701355 \t Time: 7.554845809936523 secs [1694.2768022035314 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.6550976037979126 \t Time: 7.556173324584961 secs [1693.9791413139756 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.6589277982711792 \t Time: 7.494822025299072 secs [1707.8457576168034 ej/sec]]\n",
      "Epoch [1/10]\t Loss: 0.6253055096021853\n",
      "Starting epoch 2 / 10\n",
      "Step [0/512]\t Loss: 0.7038164734840393 \t Time: 1.8526825904846191 secs [6908.900675021626 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.6743229627609253 \t Time: 7.948792219161987 secs [1610.3075344130027 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.5299764275550842 \t Time: 7.515478849411011 secs [1703.151623000461 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.6408990025520325 \t Time: 7.514791011810303 secs [1703.307514458276 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.6470080018043518 \t Time: 7.5265820026397705 secs [1700.639147425844 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.5741930603981018 \t Time: 7.5394206047058105 secs [1697.7431915671002 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.6891894340515137 \t Time: 7.54326605796814 secs [1696.8777054441878 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.601068377494812 \t Time: 7.544005632400513 secs [1696.7113525241396 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.6073972582817078 \t Time: 7.555090427398682 secs [1694.2219451908281 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.5919311046600342 \t Time: 7.604996919631958 secs [1683.1039032977615 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.5654817223548889 \t Time: 7.574958801269531 secs [1689.7781672231372 ej/sec]]\n",
      "Epoch [2/10]\t Loss: 0.6294131777831353\n",
      "Starting epoch 3 / 10\n",
      "Step [0/512]\t Loss: 0.6354222297668457 \t Time: 2.259193181991577 secs [5665.739478160182 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.6291795969009399 \t Time: 7.596879005432129 secs [1684.9024436018256 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.6017898321151733 \t Time: 7.520028114318848 secs [1702.121296013187 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.47592806816101074 \t Time: 7.526621580123901 secs [1700.6302048985556 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.6566442251205444 \t Time: 7.5255959033966064 secs [1700.8619867860352 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.6368071436882019 \t Time: 7.543021202087402 secs [1696.932788211946 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.6737850904464722 \t Time: 7.582241535186768 secs [1688.1551373164884 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.6262488961219788 \t Time: 7.606407880783081 secs [1682.791693611129 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.4966840445995331 \t Time: 7.613044261932373 secs [1681.3247840951149 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.6534161567687988 \t Time: 7.618379354476929 secs [1680.147365263204 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.6467213034629822 \t Time: 7.551838159561157 secs [1694.9515772917223 ej/sec]]\n",
      "Epoch [3/10]\t Loss: 0.6226468643872067\n",
      "Starting epoch 4 / 10\n",
      "Step [0/512]\t Loss: 0.6499543190002441 \t Time: 2.2785117626190186 secs [5617.70196230505 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.558260977268219 \t Time: 7.602778434753418 secs [1683.5950317175254 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.5641418099403381 \t Time: 7.534769773483276 secs [1698.7911223308208 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.6606448888778687 \t Time: 7.537089824676514 secs [1698.268204007953 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.5726418495178223 \t Time: 7.542562007904053 secs [1697.0360981569045 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.6839039921760559 \t Time: 7.544747352600098 secs [1696.5445497109745 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.7096723914146423 \t Time: 7.544590950012207 secs [1696.5797198030052 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.6248631477355957 \t Time: 7.5469605922698975 secs [1696.0470170084918 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.5315080285072327 \t Time: 7.5600059032440186 secs [1693.120371044616 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.5743698477745056 \t Time: 7.566693067550659 secs [1691.6240536955418 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.5051189661026001 \t Time: 7.50666618347168 secs [1705.1510866679116 ej/sec]]\n",
      "Epoch [4/10]\t Loss: 0.6201285399729386\n",
      "Starting epoch 5 / 10\n",
      "Step [0/512]\t Loss: 0.7833821177482605 \t Time: 1.8350763320922852 secs [6975.186686324879 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.6793810725212097 \t Time: 7.843021631240845 secs [1632.024059326088 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.627310037612915 \t Time: 7.539995193481445 secs [1697.613814272188 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.6438233256340027 \t Time: 7.546695232391357 secs [1696.106654083605 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.5006378889083862 \t Time: 7.54261326789856 secs [1697.024565010768 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.6784454584121704 \t Time: 7.541725397109985 secs [1697.2243519904614 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.5452289581298828 \t Time: 7.557410478591919 secs [1693.7018356034657 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.6769165992736816 \t Time: 7.54943323135376 secs [1695.4915167459149 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.5879767537117004 \t Time: 7.55285120010376 secs [1694.7242386854061 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.6148982048034668 \t Time: 7.55251145362854 secs [1694.800475125443 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.5765485167503357 \t Time: 7.49034857749939 secs [1708.8657313560175 ej/sec]]\n",
      "Epoch [5/10]\t Loss: 0.6214417074224912\n",
      "Starting epoch 6 / 10\n",
      "Step [0/512]\t Loss: 0.5646849274635315 \t Time: 2.225271224975586 secs [5752.107813347756 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.6038954257965088 \t Time: 7.7209343910217285 secs [1657.830432400054 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.511404275894165 \t Time: 7.597638130187988 secs [1684.7340950790044 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.5078800320625305 \t Time: 7.59110951423645 secs [1686.1830244965824 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.6919132471084595 \t Time: 7.556992530822754 secs [1693.7955076430944 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.6106780767440796 \t Time: 7.5379228591918945 secs [1698.0805241846463 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.5993583798408508 \t Time: 7.53652811050415 secs [1698.3947797076223 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.6515210866928101 \t Time: 7.543660879135132 secs [1696.7888940240243 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.6178776621818542 \t Time: 7.539242506027222 secs [1697.7832971637513 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.5614804625511169 \t Time: 7.543326139450073 secs [1696.864190063132 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.5614076852798462 \t Time: 7.491618871688843 secs [1708.5759725940895 ej/sec]]\n",
      "Epoch [6/10]\t Loss: 0.6191417271620594\n",
      "Starting epoch 7 / 10\n",
      "Step [0/512]\t Loss: 0.4764939546585083 \t Time: 1.8732163906097412 secs [6833.166773558679 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.6664619445800781 \t Time: 7.88777756690979 secs [1622.7638129271793 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.52885901927948 \t Time: 7.524894952774048 secs [1701.0204235849549 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.5796812772750854 \t Time: 7.52672553062439 secs [1700.6067177446496 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.6567423343658447 \t Time: 7.527925968170166 secs [1700.3355312102428 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.644783616065979 \t Time: 7.532052993774414 secs [1699.403869115072 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.5700768232345581 \t Time: 7.532351493835449 secs [1699.3365233255042 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.7539203763008118 \t Time: 7.531947374343872 secs [1699.4276996146752 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.6639404892921448 \t Time: 7.542503595352173 secs [1697.0492407703446 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.6140561699867249 \t Time: 7.547070503234863 secs [1696.0223168067132 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.6127629280090332 \t Time: 7.493706464767456 secs [1708.0999983360314 ej/sec]]\n",
      "Epoch [7/10]\t Loss: 0.6148327873088419\n",
      "Starting epoch 8 / 10\n",
      "Step [0/512]\t Loss: 0.6041219234466553 \t Time: 2.3002707958221436 secs [5564.562234693386 ej/sec]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [50/512]\t Loss: 0.5511623024940491 \t Time: 7.610683441162109 secs [1681.8463281197137 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.5964354276657104 \t Time: 7.53943395614624 secs [1697.74018506592 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.6695919632911682 \t Time: 7.532733201980591 secs [1699.2504124046873 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.4962494969367981 \t Time: 7.540668725967407 secs [1697.4621834163472 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.5630505084991455 \t Time: 7.551358938217163 secs [1695.0591416360369 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.6170036196708679 \t Time: 7.5548319816589355 secs [1694.279903388308 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.6068161129951477 \t Time: 7.551878213882446 secs [1694.9425874572566 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.7377872467041016 \t Time: 7.553398132324219 secs [1694.6015257984786 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.5438460111618042 \t Time: 7.560569763183594 secs [1692.9940997740619 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.6156167387962341 \t Time: 7.504220008850098 secs [1705.706920226796 ej/sec]]\n",
      "Epoch [8/10]\t Loss: 0.6154128919588402\n",
      "Starting epoch 9 / 10\n",
      "Step [0/512]\t Loss: 0.5065377950668335 \t Time: 2.267669916152954 secs [5644.560484232592 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.7007503509521484 \t Time: 7.647627115249634 secs [1673.7217710937234 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.5925813913345337 \t Time: 7.5368547439575195 secs [1698.3211743946733 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.6020309329032898 \t Time: 7.548974990844727 secs [1695.594437062466 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.5434108972549438 \t Time: 7.5420989990234375 secs [1697.140279073155 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.5344865918159485 \t Time: 7.54789662361145 secs [1695.8366864695572 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.5898107886314392 \t Time: 7.55198335647583 secs [1694.918989595494 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.5890566110610962 \t Time: 7.553283452987671 secs [1694.6272544474696 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.7100809812545776 \t Time: 7.563051462173462 secs [1692.4385698046738 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.631685197353363 \t Time: 7.568565130233765 secs [1691.2056353810694 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.5511234402656555 \t Time: 7.506446599960327 secs [1705.200966868618 ej/sec]]\n",
      "Epoch [9/10]\t Loss: 0.6120752710849047\n",
      "Starting epoch 10 / 10\n",
      "Step [0/512]\t Loss: 0.5890218019485474 \t Time: 2.2799346446990967 secs [5614.196016434204 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.6580511331558228 \t Time: 7.652188301086426 secs [1672.7241275783438 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.5544303059577942 \t Time: 7.542198181152344 secs [1697.1179611783068 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.8211338520050049 \t Time: 7.546645641326904 secs [1696.117799662502 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.5502158999443054 \t Time: 7.549669504165649 secs [1695.4384550128184 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.6098687648773193 \t Time: 7.550941228866577 secs [1695.1529103506643 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.5512053370475769 \t Time: 7.547067880630493 secs [1696.0229061740822 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.519984245300293 \t Time: 7.554530620574951 secs [1694.3474906487086 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.5756913423538208 \t Time: 7.563333988189697 secs [1692.3753492821374 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.6409029960632324 \t Time: 7.564641237258911 secs [1692.0828891335698 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.5592001080513 \t Time: 7.517410516738892 secs [1702.71398262719 ej/sec]]\n",
      "Epoch [10/10]\t Loss: 0.6123423793469556\n",
      "Starting epoch 1 / 10\n",
      "Step [0/512]\t Loss: 0.9379014372825623 \t Time: 2.3199121952056885 secs [5517.450197663677 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.7437820434570312 \t Time: 18.30253005027771 secs [699.3568629494358 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.5362622141838074 \t Time: 18.520273208618164 secs [691.1345127480997 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.47632166743278503 \t Time: 18.75195813179016 secs [682.5953807085449 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.5925582051277161 \t Time: 18.70800495147705 secs [684.1990919501762 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.5640436410903931 \t Time: 18.46646547317505 secs [693.148346043463 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.5168545842170715 \t Time: 18.486278772354126 secs [692.4054406851287 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.4891435205936432 \t Time: 18.47627544403076 secs [692.7803192139231 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.5720807909965515 \t Time: 18.456247568130493 secs [693.5320927371242 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.6619454622268677 \t Time: 18.515016078948975 secs [691.330752585909 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.5214115977287292 \t Time: 18.49170184135437 secs [692.2023786569177 ej/sec]]\n",
      "Epoch [1/10]\t Loss: 0.5528316267300397\n",
      "Starting epoch 2 / 10\n",
      "Step [0/512]\t Loss: 0.5397937893867493 \t Time: 1.901818037033081 secs [6730.402042021095 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.5394778251647949 \t Time: 18.388681411743164 secs [696.0803612501446 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.5436087846755981 \t Time: 18.46167778968811 secs [693.3281008267581 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.5738285183906555 \t Time: 18.5330491065979 secs [690.6580739292979 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.6218970417976379 \t Time: 18.576515436172485 secs [689.0420350350333 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.584976077079773 \t Time: 18.426626682281494 secs [694.6469487173204 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.42737871408462524 \t Time: 18.6780104637146 secs [685.297827885165 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.5000276565551758 \t Time: 18.53743863105774 secs [690.4945313509926 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.4482886791229248 \t Time: 18.52238392829895 secs [691.0557544617055 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.4917812645435333 \t Time: 18.52653217315674 secs [690.9010213226001 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.4957233965396881 \t Time: 18.485908031463623 secs [692.4193271011616 ej/sec]]\n",
      "Epoch [2/10]\t Loss: 0.5056131089804694\n",
      "Starting epoch 3 / 10\n",
      "Step [0/512]\t Loss: 0.4390718340873718 \t Time: 2.3654098510742188 secs [5411.3243817713255 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.4734290540218353 \t Time: 18.38886523246765 secs [696.073403017829 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.46914902329444885 \t Time: 18.514859676361084 secs [691.3365925393671 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.5861486792564392 \t Time: 18.531853199005127 secs [690.702643850382 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.48437637090682983 \t Time: 18.611408710479736 secs [687.7501966195906 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.6165313720703125 \t Time: 18.55150532722473 secs [689.970963230446 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.3854227066040039 \t Time: 18.595861434936523 secs [688.3251977750442 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.4549909234046936 \t Time: 18.555800676345825 secs [689.8112468041821 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.4217073917388916 \t Time: 18.500180959701538 secs [691.8851241445641 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.4616701900959015 \t Time: 18.44487166404724 secs [693.959829763943 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.48869168758392334 \t Time: 18.601979732513428 secs [688.0988036788121 ej/sec]]\n",
      "Epoch [3/10]\t Loss: 0.4798523574718274\n",
      "Starting epoch 4 / 10\n",
      "Step [0/512]\t Loss: 0.5077928304672241 \t Time: 2.3266072273254395 secs [5501.573213418705 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.4850051701068878 \t Time: 18.42329740524292 secs [694.772478479198 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.38827648758888245 \t Time: 18.459821224212646 secs [693.3978311345185 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.43578508496284485 \t Time: 18.524730920791626 secs [690.9682010891531 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.47821566462516785 \t Time: 18.542383432388306 secs [690.310393303701 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.6275762319564819 \t Time: 18.53133535385132 secs [690.7219450507548 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.582937479019165 \t Time: 18.58707022666931 secs [688.6507579679857 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.491995632648468 \t Time: 18.466713190078735 secs [693.1390479859089 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.4255765676498413 \t Time: 18.493255138397217 secs [692.1442387621413 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.41395556926727295 \t Time: 18.457128286361694 secs [693.4989994872686 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.43582814931869507 \t Time: 18.529227018356323 secs [690.8005383775287 ej/sec]]\n",
      "Epoch [4/10]\t Loss: 0.46558841021033004\n",
      "Starting epoch 5 / 10\n",
      "Step [0/512]\t Loss: 0.38557639718055725 \t Time: 2.351372003555298 secs [5443.630348854317 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.44622790813446045 \t Time: 18.41918921470642 secs [694.9274395737302 ej/sec]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [100/512]\t Loss: 0.5170642137527466 \t Time: 18.543452501296997 secs [690.2705954624535 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.3893302083015442 \t Time: 18.580814123153687 secs [688.8826245804713 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.5456705093383789 \t Time: 18.522735118865967 secs [691.0426520629134 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.37861368060112 \t Time: 18.45746088027954 secs [693.4865029932623 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.3915737271308899 \t Time: 18.597198724746704 secs [688.2757015962541 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.4708123505115509 \t Time: 18.5426926612854 secs [690.2988812797747 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.4391838610172272 \t Time: 18.686509370803833 secs [684.9861440681356 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.476293683052063 \t Time: 18.476900339126587 secs [692.7568891463244 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.3648203909397125 \t Time: 18.53321099281311 secs [690.6520410825539 ej/sec]]\n",
      "Epoch [5/10]\t Loss: 0.4513296154909767\n",
      "Starting epoch 6 / 10\n",
      "Step [0/512]\t Loss: 0.5240684747695923 \t Time: 1.912806749343872 secs [6691.737157656222 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.42234787344932556 \t Time: 18.507333278656006 secs [691.6177391565043 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.3703935444355011 \t Time: 18.56064748764038 secs [689.6311138134366 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.45995333790779114 \t Time: 18.486088514328003 secs [692.4125668920773 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.4816109538078308 \t Time: 18.570923328399658 secs [689.2495205354463 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.347091943025589 \t Time: 18.5986225605011 secs [688.2230099762362 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.5258176922798157 \t Time: 18.510627031326294 secs [691.4946737535165 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.47107353806495667 \t Time: 18.65618634223938 secs [686.0994934972097 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.36821576952934265 \t Time: 18.506065368652344 secs [691.6651241101786 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.4896102547645569 \t Time: 18.69491171836853 secs [684.6782799954849 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.47706055641174316 \t Time: 18.5420925617218 secs [690.3212222348761 ej/sec]]\n",
      "Epoch [6/10]\t Loss: 0.4392720257746987\n",
      "Starting epoch 7 / 10\n",
      "Step [0/512]\t Loss: 0.4664202034473419 \t Time: 2.3109705448150635 secs [5538.798419009848 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.43473297357559204 \t Time: 18.434789180755615 secs [694.3393751072638 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.4747459888458252 \t Time: 18.515440940856934 secs [691.314889064024 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.4375860393047333 \t Time: 18.5709867477417 secs [689.2471667697747 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.4474829137325287 \t Time: 18.537763595581055 secs [690.4824270739542 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.4150223731994629 \t Time: 18.469354391098022 secs [693.039925974317 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.5028671622276306 \t Time: 18.59394860267639 secs [688.3960084818984 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.45210179686546326 \t Time: 18.48047947883606 secs [692.6227219731298 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.4111400544643402 \t Time: 18.53366231918335 secs [690.635222524331 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.4875415563583374 \t Time: 18.54115343093872 secs [690.3561878001216 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.38333660364151 \t Time: 18.530427932739258 secs [690.7557691846484 ej/sec]]\n",
      "Epoch [7/10]\t Loss: 0.429405806644354\n",
      "Starting epoch 8 / 10\n",
      "Step [0/512]\t Loss: 0.4401843547821045 \t Time: 1.8501060009002686 secs [6918.52250291144 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.4342684745788574 \t Time: 18.521180868148804 secs [691.1006426168206 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.47139981389045715 \t Time: 18.515368223190308 secs [691.3176041494078 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.42764776945114136 \t Time: 18.548948049545288 secs [690.0660870800045 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.4522319436073303 \t Time: 18.551926374435425 secs [689.9553039213445 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.4347584545612335 \t Time: 18.692145347595215 secs [684.7796099363601 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.32413971424102783 \t Time: 18.59326195716858 secs [688.4214308111223 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.38320475816726685 \t Time: 18.602773904800415 secs [688.0694280059481 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.45064711570739746 \t Time: 18.647335290908813 secs [686.4251540669417 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.3988043963909149 \t Time: 18.59141254425049 secs [688.489912723629 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.4380193054676056 \t Time: 18.592923402786255 secs [688.4339661229308 ej/sec]]\n",
      "Epoch [8/10]\t Loss: 0.4230439927778207\n",
      "Starting epoch 9 / 10\n",
      "Step [0/512]\t Loss: 0.44546860456466675 \t Time: 1.944627046585083 secs [6582.239006948813 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.37656649947166443 \t Time: 18.521061182022095 secs [691.1051086222112 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.5614753365516663 \t Time: 18.558677196502686 secs [689.7043288415035 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.3383641242980957 \t Time: 18.588919401168823 secs [688.582252887447 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.41017401218414307 \t Time: 18.5939679145813 secs [688.3952935060355 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.3963374197483063 \t Time: 18.512470245361328 secs [691.425824341692 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.40297991037368774 \t Time: 18.656025409698486 secs [686.1054119998045 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.4240969717502594 \t Time: 18.596282243728638 secs [688.3096219039501 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.42225247621536255 \t Time: 18.560393810272217 secs [689.640539465055 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.4302296042442322 \t Time: 18.54942750930786 secs [690.0482504690307 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.4518442451953888 \t Time: 18.542577505111694 secs [690.3031682877627 ej/sec]]\n",
      "Epoch [9/10]\t Loss: 0.41735135222552344\n",
      "Starting epoch 10 / 10\n",
      "Step [0/512]\t Loss: 0.42298999428749084 \t Time: 1.867175817489624 secs [6855.273017197337 ej/sec]]\n",
      "Step [50/512]\t Loss: 0.31577596068382263 \t Time: 18.478463888168335 secs [692.6982717538428 ej/sec]]\n",
      "Step [100/512]\t Loss: 0.4410521388053894 \t Time: 18.49440860748291 secs [692.1010707431363 ej/sec]]\n",
      "Step [150/512]\t Loss: 0.4343022406101227 \t Time: 18.53445529937744 secs [690.6056743102638 ej/sec]]\n",
      "Step [200/512]\t Loss: 0.41723114252090454 \t Time: 18.488788843154907 secs [692.3114384931134 ej/sec]]\n",
      "Step [250/512]\t Loss: 0.46635866165161133 \t Time: 18.402816772460938 secs [695.545695980339 ej/sec]]\n",
      "Step [300/512]\t Loss: 0.5310819745063782 \t Time: 18.466615200042725 secs [693.1427260135027 ej/sec]]\n",
      "Step [350/512]\t Loss: 0.43575477600097656 \t Time: 18.4396755695343 secs [694.1553798890002 ej/sec]]\n",
      "Step [400/512]\t Loss: 0.40447863936424255 \t Time: 18.471076250076294 secs [692.9753213458328 ej/sec]]\n",
      "Step [450/512]\t Loss: 0.4460982084274292 \t Time: 18.466410398483276 secs [693.1504133067094 ej/sec]]\n",
      "Step [500/512]\t Loss: 0.38231340050697327 \t Time: 18.365477323532104 secs [696.9598325440237 ej/sec]]\n",
      "Epoch [10/10]\t Loss: 0.41029482867452316\n",
      "Performing final validation in test examples...\n",
      "Val accuracy:  0.8118279741045598\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def run_epoch(model, loss_fn, loader, optimizer, device):\n",
    "  \"\"\"\n",
    "  Train the model for one epoch.\n",
    "  \"\"\"\n",
    "  loss_epoch = 0\n",
    "  start_time = time.time()\n",
    "  # Set the model to training mode\n",
    "  model.train()\n",
    "  for step, (x, y) in enumerate(loader):\n",
    "    \n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # Run the model forward to compute scores and loss.\n",
    "    scores = model(x)\n",
    "    loss = loss_fn(scores, y)\n",
    "    loss_epoch = loss_epoch + loss.item()\n",
    "    # Run the model backward and take a step using the optimizer.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50== 0:\n",
    "      spent = time.time()-start_time\n",
    "      print(f\"Step [{step}/{len(loader)}]\\t Loss: {loss.item()} \\t Time: {spent} secs [{(batch_size*50)/spent} ej/sec]]\")\n",
    "      start_time = time.time()\n",
    "  return loss_epoch\n",
    "\n",
    "def check_accuracy(model, loader, device):\n",
    "  \"\"\"\n",
    "  Check the accuracy of the model.\n",
    "  \"\"\"\n",
    "  # Set the model to eval mode\n",
    "  accuracy_epoch = 0\n",
    "  model.eval()\n",
    "  num_correct, num_examples = 0, 0\n",
    "  for x, y in loader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    # Run the model forward, and compare the argmax score with the ground-truth\n",
    "    # category.\n",
    "    output = model(x)\n",
    "    #_, preds = scores.data.cpu().max(1)\n",
    "    #num_correct += (preds == y).sum()\n",
    "    #num_examples += x.size(0)\n",
    "    predicted = output.argmax(1)\n",
    "    acc = (predicted == y).sum().item() / y.size(0)\n",
    "    accuracy_epoch += acc\n",
    "\n",
    "  # Return the fraction of datapoints that were correctly classified.\n",
    "  #acc = float(num_correct) / num_examples\n",
    "  accuracy_epoch = float(accuracy_epoch) / len(loader)\n",
    "  return accuracy_epoch\n",
    "\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False\n",
    "for param in model.fc.parameters():\n",
    "  param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "\n",
    "#First phase of finetuning\n",
    "for epoch in range(num_epochs_ft1):\n",
    "  # Run an epoch over the training data.\n",
    "  print('Starting epoch %d / %d' % (epoch + 1,num_epochs_ft1))\n",
    "  loss_epoch = run_epoch(model, loss_fn, train_loader, optimizer, device)\n",
    "\n",
    "  # Check accuracy on the train and val sets.\n",
    "  #train_acc = check_accuracy(model, train_loader, device)\n",
    "  print(f\"Epoch [{epoch+1}/{num_epochs_ft1}]\\t Loss: {loss_epoch / len(train_loader)}\")\n",
    "\n",
    "#Allow updating all the weights in the second phase\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = True\n",
    "\n",
    "#Lower learning rate this time\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Train the entire model for a few more epochs, checking accuracy on the\n",
    "# train sets after each epoch.\n",
    "for epoch in range(num_epochs_ft2):\n",
    "  print('Starting epoch %d / %d' % (epoch + 1, num_epochs_ft2))\n",
    "  loss_epoch = run_epoch(model, loss_fn, train_loader, optimizer, device)\n",
    "\n",
    "  print(f\"Epoch [{epoch+1}/{num_epochs_ft2}]\\t Loss: {loss_epoch / len(train_loader)}\")\n",
    "    \n",
    "print(\"Performing final validation in test examples...\")\n",
    "val_acc = check_accuracy(model, val_loader, device)\n",
    "print('Val accuracy: ', val_acc)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNshTTuV/wHM/CMmYWttHqR",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "IFCB_FT_Baseline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
